{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f67d596-88ae-4393-bbb4-d4301d878057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pprint import pformat\n",
    "import traceback\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima.arima import ARIMA, auto_arima\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, \n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "\n",
    "sys.path.append(\"/tf/correlation-coef-predict/ywt_library\")\n",
    "import data_generation\n",
    "from data_generation import data_gen_cfg\n",
    "from ywt_arima import arima_model, arima_err_logger_init\n",
    "\n",
    "with open('../config/data_config.yaml') as f:\n",
    "    data = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data))\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501\n",
    "logging.info(pformat(data_cfg, indent=1, width=100, compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a012b1-6376-486e-b5dd-93c76929fc3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe4cd793-e4d8-43b3-b592-bad9cf4342f6",
   "metadata": {},
   "source": [
    "## Data implement & output setting & trainset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7765e10-2944-4621-b62f-6602a573387f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = Path('./save_models/')\n",
    "lstm_log_dir = Path('./save_models/lstm_train_logs/')\n",
    "res_dir = Path('./results/')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "lstm_log_dir.mkdir(parents=True, exist_ok=True)\n",
    "res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# setting of output files\n",
    "save_corr_data = True\n",
    "save_arima_resid_data = True\n",
    "# data implement setting\n",
    "data_implement = \"SP500_20082017_FREQ_CLUSTER_TEST\"  # watch options by operate: print(data_cfg[\"DATASETS\"].keys())\n",
    "# train set setting\n",
    "train_items_setting = \"-train_train\"  # -train_train|-train_all\n",
    "# data split  period setting, only suit for only settings of Korean paper\n",
    "data_split_settings = [\"-data_sp_train\", \"-data_sp_dev\", \"-data_sp_test1\", \"-data_sp_test2\", ]\n",
    "# lstm_hyper_params\n",
    "lstm_hyper_param = \"-kS_hyper\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eca3555-279b-42f5-88e3-fe8a7745242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading & implement setting\n",
    "dataset_df = pd.read_csv(data_cfg[\"DATASETS\"][data_implement]['FILE_PATH'])\n",
    "dataset_df = dataset_df.set_index('Date')\n",
    "all_set = list(dataset_df.columns)  # all data\n",
    "train_set = data_cfg[\"DATASETS\"][data_implement]['TRAIN_SET']\n",
    "test_set = data_cfg['DATASETS'][data_implement]['TEST_SET'] if data_cfg['DATASETS'][data_implement].get('TEST_SET') else [p for p in all_set if p not in train_set]  # all data - train data\n",
    "logging.info(f\"===== len(train_set): {len(train_set)}, len(all_set): {len(all_set)}, len(test_set): {len(test_set)} =====\")\n",
    "\n",
    "# train items implement settings\n",
    "items_implement = train_set if train_items_setting == \"-train_train\" else all_set\n",
    "logging.info(f\"===== len(train set): {len(items_implement)} =====\")\n",
    "\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + train_items_setting\n",
    "logging.info(f\"===== file_name basis:{output_file_name} =====\")\n",
    "\n",
    "# display(dataset_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f530b-2393-4a1d-82c0-38382b1d6133",
   "metadata": {},
   "source": [
    "## Load or Create Correlation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916be840-68c4-4b6a-b188-5a29cc991d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-corr_data\"\n",
    "corr_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_length = int(len(dataset_df)/data_gen_cfg[\"CORR_WINDOW\"])*data_gen_cfg[\"CORR_WINDOW\"]\n",
    "corr_ser_len_max = int((data_length-data_gen_cfg[\"CORR_WINDOW\"])/data_gen_cfg[\"CORR_STRIDE\"])\n",
    "\n",
    "train_df_path = corr_data_dir/f\"{output_file_name}-corr_train.csv\"\n",
    "dev_df_path = corr_data_dir/f\"{output_file_name}-corr_dev.csv\"\n",
    "test1_df_path = corr_data_dir/f\"{output_file_name}-corr_test1.csv\"\n",
    "test2_df_path = corr_data_dir/f\"{output_file_name}-corr_test2.csv\"\n",
    "all_corr_df_paths = dict(zip([\"train_df\", \"dev_df\", \"test1_df\", \"test2_df\"],\n",
    "                             [train_df_path, dev_df_path, test1_df_path, test2_df_path]))\n",
    "if all([df_path.exists() for df_path in all_corr_df_paths.values()]):\n",
    "    corr_datasets = [pd.read_csv(df_path).set_index(\"items\") for df_path in all_corr_df_paths.values()]\n",
    "else:\n",
    "    corr_datasets = data_generation.gen_train_data(items_implement, raw_data_df=dataset_df, corr_df_paths=all_corr_df_paths, corr_ser_len_max=corr_ser_len_max, save_file=save_corr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7976949-33c7-45c0-b6e9-c8cace53672a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM model for first stage prediciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ee34a-a8f6-4609-9e69-cfa66bc19100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_log = keras.callbacks.TensorBoard(lstm_log_dir=lstm_log_dir)\n",
    "model_earlystop = tf.keras.callbacks.EarlyStopping(patience=50, monitor=\"val_loss\")\n",
    "save_model = keras.callbacks.ModelCheckpoint(Path(model_dir)/\"epoch_{epoch}_{val_loss:.5f}.h5\",\n",
    "                                             monitor='val_loss', verbose=1, mode='min', save_best_only=True)\n",
    "callbacks_list = [model_log, model_earlystop, save_model]\n",
    "inputs = Input(shape=(20, 1))\n",
    "lstm_1 = LSTM(units=20, kernel_regularizer=l1_l2(0.0, 0.0), bias_regularizer=l1_l2(0.0, 0.0))(inputs)\n",
    "outputs = Dense(units=21, activation=\"relu\")(lstm_1)\n",
    "first_stage_lstm_model = keras.Model(inputs, outputs, name=\"first_stage_lstm\")\n",
    "first_stage_lstm_model.summary()\n",
    "first_stage_lstm_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])\n",
    "train_history = first_stage_lstm_model.fit(x=corr_datasets[0], y=corr_datasets[0], validation_data=(corr_datasets[1], corr_datasets[1]), epochs=500, batch_size=64, shuffle=True, callbacks=callbacks_list, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfd3c44-28e6-43e9-8b83-a7d86f121938",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_epoch_num = np.argmin(np.array(train_history.history['val_loss'])) + 1\n",
    "best_val_loss = train_history.history['val_loss'][epoch_num-1]\n",
    "best_first_stage_lstm_model = load_model(Path(model_dir)/f\"epoch_{best_epoch_num}_{best_val_loss:.5f}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a253c7-552c-4c6c-9072-8a6cc83297c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for (file_name, dataset) in tqdm(zip(['train', 'dev', 'test1', 'test2'], corr_datasets)):\n",
    "    (dataset, save_file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb455ad-4103-403d-a65f-ba8a13038295",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM for residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed36f1c-62c7-4874-a3a4-1aaf35bcdc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset.from_tensor_slices(dict(pd.read_csv(f'./dataset/after_arima/arima_resid_train.csv')))\n",
    "lstm_train_X = pd.read_csv(f'./dataset/after_arima/{output_file_name}_arima_resid_train.csv').set_index('Unnamed: 0').iloc[::, :-1]\n",
    "lstm_train_Y = pd.read_csv(f'./dataset/after_arima/{output_file_name}_arima_resid_train.csv').set_index('Unnamed: 0').iloc[::, -1]\n",
    "lstm_dev_X = pd.read_csv(f'./dataset/after_arima/{output_file_name}_arima_resid_dev.csv').set_index('Unnamed: 0').iloc[::, :-1]\n",
    "lstm_dev_Y = pd.read_csv(f'./dataset/after_arima/{output_file_name}_arima_resid_dev.csv').set_index('Unnamed: 0').iloc[::, -1]\n",
    "lstm_test1_X = pd.read_csv(f'./dataset/after_arima/{output_file_name}_arima_resid_test1.csv').set_index('Unnamed: 0').iloc[::, :-1]\n",
    "lstm_test1_Y = pd.read_csv(f'./dataset/after_arima/{output_file_name}_arima_resid_test1.csv').set_index('Unnamed: 0').iloc[::, -1]\n",
    "lstm_test2_X = pd.read_csv(f'./dataset/after_arima/{output_file_name}_arima_resid_test2.csv').set_index('Unnamed: 0').iloc[::, :-1]\n",
    "lstm_test2_Y = pd.read_csv(f'./dataset/after_arima/{output_file_name}_arima_resid_test2.csv').set_index('Unnamed: 0').iloc[::, -1]\n",
    "num_samples = len(lstm_train_X)\n",
    "\n",
    "lstm_train_X = lstm_train_X.values.reshape(num_samples, 20, 1)\n",
    "lstm_train_Y = lstm_train_Y.values.reshape(num_samples, 1)\n",
    "lstm_dev_X = lstm_dev_X.values.reshape(num_samples, 20, 1)\n",
    "lstm_dev_Y = lstm_dev_Y.values.reshape(num_samples, 1)\n",
    "lstm_test1_X = lstm_test1_X.values.reshape(num_samples, 20, 1)\n",
    "lstm_test1_Y = lstm_test1_Y.values.reshape(num_samples, 1)\n",
    "lstm_test2_X = lstm_test2_X.values.reshape(num_samples, 20, 1)\n",
    "lstm_test2_Y = lstm_test2_Y.values.reshape(num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df49f64-a93a-4104-b07d-dfbd0a688004",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) *2)\n",
    "\n",
    "\n",
    "def build_many_one_lstm():\n",
    "    inputs = Input(shape=(20, 1))\n",
    "    lstm_1 = LSTM(units=25, kernel_regularizer=l1_l2(0.0, 0.0), bias_regularizer=l1_l2(0.0, 0.0))(inputs)\n",
    "    outputs = Dense(units=1, activation=double_tanh)(lstm_1)\n",
    "    return keras.Model(inputs, outputs, name=\"many_one_lstm\")\n",
    "\n",
    "\n",
    "lstm_model = build_many_one_lstm()\n",
    "lstm_model.summary()\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa2b47f-ad31-431f-8ded-1e2acf98d7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = './models'\n",
    "log_dir = './models/lstm_train_logs'\n",
    "res_dir = './results'\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "res_csv_path = Path(res_dir+f'/{output_file_name}_LSTM_evaluation.csv')\n",
    "res_csv_path.touch(exist_ok=True)\n",
    "with open(res_csv_path, 'r+') as f:\n",
    "    if not f.read():\n",
    "        f.write(\"epoch,TRAIN_MSE,DEV_MSE,TEST1_MSE,TEST2_MSE,TRAIN_MAE,DEV_MAE,TEST1_MAE,TEST2_MAE\")\n",
    "\n",
    "res_df = pd.read_csv(res_csv_path)\n",
    "saved_model_list = [int(p.stem.split('_')[1]) for p in Path(model_dir).glob('*.h5')]\n",
    "model_log = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "epoch_start = max(saved_model_list) if saved_model_list else 1\n",
    "max_epoch = 3000\n",
    "\n",
    "for epoch_num in range(epoch_start, max_epoch):\n",
    "    if epoch_num > 1:\n",
    "        lstm_model = load_model(Path(model_dir)/f\"{output_file_name}_epoch_{epoch_num - 1}.h5\", custom_objects={'double_tanh':double_tanh})\n",
    "\n",
    "    save_model = keras.callbacks.ModelCheckpoint(Path(model_dir)/f\"{output_file_name}_epoch_{epoch_num}.h5\",\n",
    "                                                 monitor='loss', verbose=1, mode='min', save_best_only=False)\n",
    "    lstm_model.fit(lstm_train_X, lstm_train_Y, epochs=1, batch_size=500, shuffle=True, callbacks=[model_log, save_model])\n",
    "    \n",
    "    # test the model\n",
    "    score_train = lstm_model.evaluate(lstm_train_X, lstm_train_Y)\n",
    "    score_dev = lstm_model.evaluate(lstm_dev_X, lstm_dev_Y)\n",
    "    score_test1 = lstm_model.evaluate(lstm_test1_X, lstm_test1_Y)\n",
    "    score_test2 = lstm_model.evaluate(lstm_test2_X, lstm_test2_Y)\n",
    "    res_each_epoch_df = pd.DataFrame(np.array([epoch_num, score_train[0], score_dev[0], \n",
    "                                               score_test1[0], score_test2[0], \n",
    "                                               score_train[1], score_dev[1], \n",
    "                                               score_test1[1], score_test2[1]]).reshape(-1, 9),\n",
    "                                    columns=[\"epoch\", \"TRAIN_MSE\", \"DEV_MSE\", \"TEST1_MSE\", \n",
    "                                             \"TEST2_MSE\", \"TRAIN_MAE\", \"DEV_MAE\",\n",
    "                                             \"TEST1_MAE\",\"TEST2_MAE\"])\n",
    "    res_df = pd.concat([res_df, res_each_epoch_df])\n",
    "\n",
    "res_df.to_csv(res_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2bb3d3-1516-4da3-ab8e-f27bd25b0945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
