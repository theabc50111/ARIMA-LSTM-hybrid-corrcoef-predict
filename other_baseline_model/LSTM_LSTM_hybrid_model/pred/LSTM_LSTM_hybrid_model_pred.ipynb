{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41660f84-97c9-4162-a708-f601b3b7a3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 122 ms (started: 2022-11-30 17:58:05 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pprint import pformat\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima.arima import ARIMA, auto_arima\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "sys.path.append(\"/tf/correlation-coef-predict/ywt_library\")\n",
    "import data_generation\n",
    "from data_generation import data_gen_cfg\n",
    "from ywt_arima import arima_model, arima_err_logger_init\n",
    "from stl_decompn import stl_decompn\n",
    "from corr_property import calc_corr_ser_property\n",
    "\n",
    "with open('../../../config/data_config.yaml') as f:\n",
    "    data = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data))\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "matplotlib_logger = logging.getLogger(\"matplotlib\")\n",
    "matplotlib_logger.setLevel(logging.ERROR)\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# logger_list = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "# print(logger_list)\n",
    "\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501\n",
    "logging.debug(pformat(data_cfg, indent=1, width=100, compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a012b1-6376-486e-b5dd-93c76929fc3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad3949-4662-4abc-8415-f86ed2ab03e1",
   "metadata": {},
   "source": [
    "## Data implement & output setting & testset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4848bc85-9efc-4bc5-a63a-066bbc4d16ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 613 Âµs (started: 2022-11-30 18:03:52 +00:00)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# setting of output files\n",
    "save_corr_data = True\n",
    "save_lstm_resid_data = True\n",
    "# data implement setting\n",
    "data_implement = \"SP500_20082017\"  # watch options by operate: print(data_cfg[\"DATASETS\"].keys())\n",
    "# data_implement = \"SP500_20082017_FREQ_CLUSTER_TEST\"  # watch options by operate: print(data_cfg[\"DATASETS\"].keys())\n",
    "# test set setting\n",
    "test_items_setting = \"-test_test\"  # -test_test|-test_all\n",
    "# data split period setting, only suit for only settings of Korean paper\n",
    "data_split_setting = \"-data_sp_test2\"\n",
    "# lstm weight setting\n",
    "lstm_weight_setting = \"\"  # watch options by operate: print(data_cfg[\"LSTM_LSTM_LSTM_WEIGHT\"].items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431c52f-e408-4b59-b512-ce32d8607d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data loading & implement setting\n",
    "dataset_df = pd.read_csv(data_cfg[\"DATASETS\"][data_implement]['FILE_PATH'])\n",
    "dataset_df = dataset_df.set_index('Date')\n",
    "all_set = list(dataset_df.columns)  # all data\n",
    "train_set = data_cfg[\"DATASETS\"][data_implement]['TRAIN_SET']\n",
    "test_set = data_cfg['DATASETS'][data_implement]['TEST_SET'] if data_cfg['DATASETS'][data_implement].get('TEST_SET') else [p for p in all_set if p not in train_set]  # all data - train data\n",
    "logging.info(f\"===== len(train_set): {len(train_set)}, len(all_set): {len(all_set)}, len(test_set): {len(test_set)} =====\")\n",
    "\n",
    "# test items implement settings\n",
    "items_implement = test_set if test_items_setting == \"-test_test\" else all_set\n",
    "logging.info(f\"===== len(test set): {len(items_implement)} =====\")\n",
    "\n",
    "lstm_weight_filepath = data_cfg[\"LSTM_WEIGHT\"][lstm_weight_setting][\"FILE_PATH\"]\n",
    "lstm_weight_name = data_cfg[\"LSTM_WEIGHT\"][lstm_weight_setting][\"LSTM_WEIGHT_NAME\"]\n",
    "logging.info(f\"===== LSTM weight:{lstm_weight_name} =====\")\n",
    "\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + test_items_setting\n",
    "fig_title = data_implement + test_items_setting + lstm_weight_name + data_split_setting\n",
    "logging.info(f\"===== file_name basis:{output_file_name}, fig_title basis:{fig_title} =====\")\n",
    "# display(dataset_df)\n",
    "# display(test_set)\n",
    "\n",
    "# output folder settings\n",
    "corr_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-corr_data\"\n",
    "first_stage_lstm_result_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-first_stage_lstm_res\"\n",
    "res_dir = Path('./results/')\n",
    "corr_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "first_stage_lstm_result_dir.mkdir(parents=True, exist_ok=True)\n",
    "res_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e76df-1712-4ec8-a318-7b2e6bf556f6",
   "metadata": {},
   "source": [
    "## Load or Create Correlation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dff757-38bd-4bc9-8926-6ea54ff2ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = int(len(dataset_df)/data_gen_cfg[\"CORR_WINDOW\"])*data_gen_cfg[\"CORR_WINDOW\"]\n",
    "corr_ser_len_max = int((data_length-data_gen_cfg[\"CORR_WINDOW\"])/data_gen_cfg[\"CORR_STRIDE\"])\n",
    "max_data_div_start_add = 0  # In the Korea paper, each pair has 5 corr_series(due to diversifing train data).\n",
    "                            # BUT we only need to take one, so take 0 as arg.\n",
    "corr_ind = []\n",
    "\n",
    "# DEFAULT SETTING: data_gen_cfg[\"DATA_DIV_STRIDE\"] == 20, data_gen_cfg[\"CORR_WINDOW\"]==100, data_gen_cfg[\"CORR_STRIDE\"]==100\n",
    "data_end_init = corr_ser_len_max * data_gen_cfg[\"CORR_STRIDE\"]\n",
    "for i in range(0, max_data_div_start_add+1, data_gen_cfg[\"DATA_DIV_STRIDE\"]):\n",
    "    corr_ind.extend(list(range(data_gen_cfg[\"CORR_WINDOW\"]-1+i, data_end_init+bool(i)*data_gen_cfg[\"CORR_STRIDE\"], data_gen_cfg[\"CORR_STRIDE\"])))  # only suit for settings of paper\n",
    "\n",
    "train_df_path = corr_data_dir/f\"{output_file_name}-corr_train.csv\"\n",
    "dev_df_path = corr_data_dir/f\"{output_file_name}-corr_dev.csv\"\n",
    "test1_df_path = corr_data_dir/f\"{output_file_name}-corr_test1.csv\"\n",
    "test2_df_path = corr_data_dir/f\"{output_file_name}-corr_test2.csv\"\n",
    "all_corr_df_paths = dict(zip([\"train_df\", \"dev_df\", \"test1_df\", \"test2_df\"],\n",
    "                             [train_df_path, dev_df_path, test1_df_path, test2_df_path]))\n",
    "if all([df_path.exists() for df_path in all_corr_df_paths.values()]):\n",
    "    corr_datasets = [pd.read_csv(df_path).set_index(\"items\") for df_path in all_corr_df_paths.values()]\n",
    "else:\n",
    "    corr_datasets = data_generation.gen_train_data(items_implement, raw_data_df=dataset_df, corr_ser_len_max=corr_ser_len_max, corr_df_paths=all_corr_df_paths, corr_ind=corr_ind, max_data_div_start_add=max_data_div_start_add, save_file=save_corr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89d9f0-ec59-4066-bc6a-6d4ed2345869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if data_split_setting == \"-data_sp_test2\":\n",
    "    corr_dataset = corr_datasets[3]\n",
    "print(corr_datasets[0].shape, corr_datasets[1].shape, corr_datasets[2].shape, corr_datasets[3].shape)\n",
    "# display(corr_dataset.iloc[0,::])\n",
    "display(corr_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7976949-33c7-45c0-b6e9-c8cace53672a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bfef6-4dde-409f-aa4a-e04cdfad0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_result_path_basis = arima_result_dir/f'{output_file_name}.csv'\n",
    "arima_result_types = [\"-arima_output\", \"-arima_resid\", \"-arima_model_info\"]\n",
    "arima_result_paths = []\n",
    "arima_err_logger_init(Path(os.path.abspath(''))/f\"results\")\n",
    "\n",
    "for arima_result_type in arima_result_types:\n",
    "    arima_result_paths.append(arima_result_dir/f'{output_file_name}{arima_result_type}{data_split_setting}.csv')\n",
    "\n",
    "if all([df_path.exists() for df_path in arima_result_paths]):\n",
    "    arima_output_df, arima_resid_df, arima_model_info_df = [pd.read_csv(arima_result_path, index_col=\"items\") for arima_result_path in arima_result_paths]\n",
    "else:\n",
    "    arima_output_df, arima_resid_df, arima_model_info_df = arima_model(corr_dataset, arima_result_path_basis=arima_result_path_basis, data_split_setting=data_split_setting, save_file=save_arima_resid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb455ad-4103-403d-a65f-ba8a13038295",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35591071-44de-4a02-8dfa-fb85bb7ae29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) * 2)\n",
    "\n",
    "\n",
    "lstm_model = load_model(lstm_weight_filepath, custom_objects={'double_tanh':double_tanh})\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65707dce-375f-4d15-84fd-e4edf9133cac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32998ac-361f-427f-aaf4-4667f8a6acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = arima_resid_df.iloc[::,:-1].values.reshape(-1, 20, 1)\n",
    "lstm_pred = lstm_model.predict(lstm_input)\n",
    "lstm_pred = pd.DataFrame(lstm_pred, index=arima_resid_df.index, columns=[\"lstm_pred\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc683e2-8879-4fb2-ba95-50403889c18d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924c24c-df58-4521-9f33-5f8a0fe8124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_df_postprocess(merge_dfs: list) -> \"pd.DataFrame\":\n",
    "\n",
    "    tmp_df = pd.DataFrame(columns=[\"items\"])\n",
    "    for merge_df in merge_dfs:\n",
    "        tmp_df = pd.merge(tmp_df, merge_df, on=\"items\", how='outer')\n",
    "    else:\n",
    "        results_df = tmp_df.reset_index(drop=True)\n",
    "\n",
    "    results_df[\"hybrid_model_pred\"] =  results_df[\"arima_pred\"] + results_df[\"lstm_pred\"]\n",
    "    results_df[\"error\"] = results_df[\"ground_truth\"] - results_df[\"hybrid_model_pred\"]\n",
    "    results_df[\"absolute_err\"] = results_df[\"error\"].abs()\n",
    "    results_df['arima_pred_dir'] = np.sign(results_df['ground_truth'] * results_df['arima_pred'])\n",
    "    results_df['arima_err'] = results_df['ground_truth'] - results_df['arima_pred']\n",
    "    results_df[\"lstm_compensation_dir\"] = np.sign(results_df['arima_err']) * np.sign(results_df['lstm_pred'])\n",
    "    quantile_mask = np.logical_and(results_df['error'] < np.quantile(results_df['error'], 0.75), results_df['error'] > np.quantile(results_df['error'], 0.25)).tolist()\n",
    "    # display(np.quantile(res_df['error'], 0.75), np.quantile(res_df['error'], 0.25))\n",
    "    results_df['high_pred_performance'] = quantile_mask\n",
    "    results_df['items[0]'] = results_df.apply(lambda row:row['items'].split(\" & \")[0], axis=1)\n",
    "    results_df['items[1]'] = results_df.apply(lambda row:row['items'].split(\" & \")[1][:-2], axis=1)\n",
    "    results_df = results_df.set_index(\"items\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# stl_decompn(corr_datasets[0].iloc[0,::], overview=True)\n",
    "corr_property_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}{lstm_weight_name}-corr_property\"\n",
    "corr_property_dir.mkdir(parents=True, exist_ok=True)\n",
    "corr_property_df_path = corr_property_dir/f\"{output_file_name}{lstm_weight_name}{data_split_setting}-corr_series_property.csv\"\n",
    "\n",
    "if corr_property_df_path.exists():\n",
    "    corr_property_df = pd.read_csv(corr_property_df_path)\n",
    "else:\n",
    "    corr_property_df = calc_corr_ser_property(corr_dataset=corr_dataset, corr_property_df_path=corr_property_df_path)\n",
    "\n",
    "ground_truth = corr_dataset.iloc[::, -1]\n",
    "ground_truth.name = \"ground_truth\"\n",
    "arima_pred = arima_output_df.iloc[::, -1]\n",
    "arima_pred.name = \"arima_pred\"\n",
    "merge_dfs = [arima_model_info_df, corr_property_df, arima_pred, lstm_pred, ground_truth]\n",
    "\n",
    "res_df = res_df_postprocess(merge_dfs)\n",
    "res_df.to_csv(res_dir/f\"{output_file_name}{lstm_weight_name}{data_split_setting}-res.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8daca-b55f-4a00-b162-4968bf696c50",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84895d7-ae0e-4cf9-9ecc-2adf76c38462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(res_dir/f\"{output_file_name}{lstm_weight_name}{data_split_setting}-res.csv\", index_col=[\"items\"])\n",
    "display(res_df.columns)\n",
    "display(res_df.shape)\n",
    "display(res_df.head())\n",
    "display(f\"mse :{(res_df['error']**2).mean()}\",\n",
    "        f\"std of square_err :{(res_df['error']**2).std()}\",\n",
    "        f\"rmse :{np.sqrt((res_df['error']**2).mean())}\",\n",
    "        f\"mae : {res_df['absolute_err'].mean()}\",\n",
    "        f\"std of abs_err: {res_df['absolute_err'].std()}\",\n",
    "        f\"sklearn mse: {mean_squared_error(res_df['ground_truth'], res_df['hybrid_model_pred'])}\")\n",
    "display(\"-\"*50)\n",
    "display(f\"mse of ARIMA :{(res_df['arima_err']**2).mean()}\",\n",
    "        f\"std of square_err ARIMA :{(res_df['arima_err']**2).std()}\",\n",
    "        f\"rmse of ARIMA :{np.sqrt((res_df['arima_err']**2).mean())}\",\n",
    "        f\"sklearn mse of ARIMA: {mean_squared_error(res_df['ground_truth'], res_df['arima_pred'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7c64a-f3c7-4b5d-93bc-737c4ae4bf67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_exploration(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=7, ncols=2, sharex=False, sharey=False, dpi=100)\n",
    "    s0 = axes[0, 0]\n",
    "    s0.set_title(\"ABS_err violin\")\n",
    "    sns.violinplot(y=target_df[\"absolute_err\"], ax=s0)\n",
    "    s1 = axes[0, 1]\n",
    "    s1.set_title(\"Err violin\")\n",
    "    sns.violinplot(y=target_df[\"error\"], ax=s1)\n",
    "    s2 = axes[1, 0]\n",
    "    s2.set_title(\"ABS_err hist\")\n",
    "    target_df['absolute_err'].hist(bins=[b/10 for b in range(11)], ax=s2)\n",
    "    s3 = axes[1, 1]\n",
    "    s3.set_title(\"Err hist\")\n",
    "    target_df['error'].hist(bins=[b/10 for b in range(-10, 11)], ax=s3)\n",
    "    s4 = axes[2, 0]\n",
    "    s4.set_title(\"LSTM_compensation_dir count\")\n",
    "    sns.countplot(x=\"lstm_compensation_dir\", data=target_df, ax=s4)\n",
    "    s5 = axes[2, 1]\n",
    "    s5.set_title(\"LSTM_compensation_dir count groupby ARIMA_pred_dir\")\n",
    "    df_gb = target_df.groupby(['arima_pred_dir', 'lstm_compensation_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s5)\n",
    "    s6 = axes[3, 0]\n",
    "    s6.set_title(\"ARIMA_model prediction Err violin group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"arima_err\"], ax=s6)\n",
    "    s8 = axes[4, 0]\n",
    "    s8.set_title(\"ARIMA_model prediction magnitude group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"arima_pred\"], ax=s8)\n",
    "    s9 = axes[4, 1]\n",
    "    s9.set_title(\"LSTM compensation magnitude group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"lstm_pred\"], ax=s9)\n",
    "    s10 = axes[5, 0]\n",
    "    s10.set_title(\"Correlation magnitude in last period group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"ground_truth\"], ax=s10)\n",
    "    s11 = axes[5, 1]\n",
    "    s11.set_title(\"Hybrid Err violin group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"error\"], ax=s11)\n",
    "    s12 = axes[6,0]\n",
    "    s12.set_title(\"LSTM_compensation_dir pie with wrong ARIMA_pred_dir\")\n",
    "    df_gb.loc[df_gb.index==-1, :].squeeze().plot(kind=\"pie\", autopct='%1.1f%%', ax=s12)\n",
    "    s13 = axes[6,1]\n",
    "    s13.set_title(\"LSTM_compensation_dir pie with correct ARIMA_pred_dir\")\n",
    "    df_gb.loc[df_gb.index==1, :].squeeze().plot(kind=\"pie\", autopct='%1.1f%%', ax=s13)\n",
    "\n",
    "    fig.suptitle(f\"{title}_basic_exploration\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./results/hybrid_prediction_analysis_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_exploration_pred_perform(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=6, ncols=2, sharex=False, sharey=False, dpi=100)\n",
    "    s1 = axes[0, 0]\n",
    "    s1.set_title(\"LSTM_compensation_dir count groupby prediction performance\")\n",
    "    df_gb = target_df.groupby(['high_pred_performance', 'lstm_compensation_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s1)\n",
    "    s2 = axes[0, 1]\n",
    "    s2.set_title(\"ARIMA_model prediction magnitude group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"arima_pred\"], ax=s2)\n",
    "    s3 = axes[1, 0]\n",
    "    s3.set_title(\"LSTM compensation magnitude group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"lstm_pred\"], ax=s3)\n",
    "    s4 = axes[1, 1]\n",
    "    s4.set_title(\"Correlation magnitude in last period group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"ground_truth\"], ax=s4)\n",
    "    s5 = axes[2, 0]\n",
    "    s5.set_title(\"Correlation series mean groupby prediction performance\")\n",
    "    sns.violinplot(x=target_df['high_pred_performance'], y=target_df[\"corr_ser_mean\"], ax=s5)\n",
    "    s6 = axes[2, 1]\n",
    "    s6.set_title(\"Correlation series std groupby prediction performance\")\n",
    "    sns.violinplot(x=target_df['high_pred_performance'], y=target_df[\"corr_ser_std\"], ax=s6)\n",
    "    s7 = axes[3, 0]\n",
    "    s7.set_title(\"Correlation series stl_period group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_period\"], ax=s7)\n",
    "    s8 = axes[3, 1]\n",
    "    s8.set_title(\"Correlation series stl_residual group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_resid\"], ax=s8)\n",
    "    s9 = axes[4, 0]\n",
    "    s9.set_title(\"Correlation series stl_trend_std group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_trend_std\"], ax=s9)\n",
    "    s10 = axes[4, 1]\n",
    "    s10.set_title(\"Correlation series stl_trend_coef group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_trend_coef\"], ax=s10)\n",
    "    s11 = axes[5, 0]\n",
    "    s11.set_title(\"ARIMA_pred_dir count groupby prediction performance\")\n",
    "    df_gb = target_df.groupby(['high_pred_performance', 'arima_pred_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s11)\n",
    "\n",
    "    fig.suptitle(F\"{title}_groupby prediction\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./results/hybrid_prediction_analysis_groupby_pred_perform_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_stock_freq(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    stocks_show_freq = target_df.loc[target_df['high_pred_performance'] == True, ['items[0]','items[1]']].stack().value_counts().to_dict()\n",
    "    plt.figure(figsize=(80, 10), dpi=100)\n",
    "    plt.bar(range(len(stocks_show_freq)), list(stocks_show_freq.values()))\n",
    "    plt.xticks(range(len(stocks_show_freq)), list(stocks_show_freq.keys()), rotation=60)\n",
    "    plt.title(F\"{title}_items appearence frequence\")\n",
    "    plt.savefig(f\"./results/items_appearence_frequence_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c7e89-18db-498a-b3bd-2ad63a185935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_exploration(res_df, fig_title)\n",
    "plot_exploration_pred_perform(res_df, fig_title)\n",
    "plot_stock_freq(res_df, fig_title)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
