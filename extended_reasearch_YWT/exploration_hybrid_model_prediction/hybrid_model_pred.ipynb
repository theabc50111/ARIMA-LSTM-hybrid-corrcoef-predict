{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "41660f84-97c9-4162-a708-f601b3b7a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 766 µs (started: 2022-09-10 02:23:06 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from collections import OrderedDict, namedtuple\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from pmdarima.arima import ARIMA, auto_arima\n",
    "from keras.models import load_model\n",
    "from keras.layers import LSTM, Dense\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a012b1-6376-486e-b5dd-93c76929fc3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a90c3941-5ffd-4aef-8d42-a55d5f602187",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:len(train_set): 150, len(all_set): 445, len(test_set): 295\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 113 ms (started: 2022-09-09 16:59:16 +00:00)\n"
     ]
    }
   ],
   "source": [
    "stock_price_df = pd.read_csv(\"../../stock08_price.csv\")\n",
    "\n",
    "universe = list(stock_price_df.columns.values[1:])\n",
    "universe.remove(\"SP500\")\n",
    "# train data\n",
    "portfolio_train = ['CELG', 'PXD', 'WAT', 'LH', 'AMGN', 'AOS', 'EFX', 'CRM', 'NEM', 'JNPR', 'LB', 'CTAS', 'MAT', 'MDLZ', 'VLO', 'APH', 'ADM', 'MLM', 'BK', 'NOV', 'BDX', 'RRC', 'IVZ', 'ED', 'SBUX', 'GRMN', 'CI', 'ZION', 'COO', 'TIF', 'RHT', 'FDX', 'LLL', 'GLW', 'GPN', 'IPGP', 'GPC', 'HPQ', 'ADI', 'AMG', 'MTB', 'YUM', 'SYK', 'KMX', 'AME', 'AAP', 'DAL', 'A', 'MON', 'BRK', 'BMY', 'KMB', 'JPM', 'CCI', 'AET', 'DLTR', 'MGM', 'FL', 'HD', 'CLX', 'OKE', 'UPS', 'WMB', 'IFF', 'CMS', 'ARNC', 'VIAB', 'MMC', 'REG', 'ES', 'ITW', 'NDAQ', 'AIZ', 'VRTX', 'CTL', 'QCOM', 'MSI', 'NKTR', 'AMAT', 'BWA', 'ESRX', 'TXT', 'EXR', 'VNO', 'BBT', 'WDC', 'UAL', 'PVH', 'NOC', 'PCAR', 'NSC', 'UAA', 'FFIV', 'PHM', 'LUV', 'HUM', 'SPG', 'SJM', 'ABT', 'CMG', 'ALK', 'ULTA', 'TMK', 'TAP', 'SCG', 'CAT', 'TMO', 'AES', 'MRK', 'RMD', 'MKC', 'WU', 'ACN', 'HIG', 'TEL', 'DE', 'ATVI', 'O', 'UNM', 'VMC', 'ETFC', 'CMA', 'NRG', 'RHI', 'RE', 'FMC', 'MU', 'CB', 'LNT', 'GE', 'CBS', 'ALGN', 'SNA', 'LLY', 'LEN', 'MAA', 'OMC', 'F', 'APA', 'CDNS', 'SLG', 'HP', 'XLNX', 'SHW', 'AFL', 'STT', 'PAYX', 'AIG', 'FOX', 'MA']\n",
    "# all data\n",
    "portfolio_all = universe\n",
    "# all data - train data\n",
    "portfolio_other = [p for p in universe if p not in portfolio_train]\n",
    "logging.info(f\"len(train_set): {len(portfolio_train)}, len(all_set): {len(portfolio_all)}, len(test_set): {len(portfolio_other)}\")\n",
    "\n",
    "\n",
    "# setting of output files\n",
    "save_raw_corr_data = True\n",
    "save_arima_resid_data = True\n",
    "time_period = \"_test2\"\n",
    "\n",
    "lstm_weight_setting = \"tetuan_power\"\n",
    "if lstm_weight_setting == \"sp500\": # lstm weight set\n",
    "    lstm_weight_filepath =  \"../rebuild_hybrid_model/models/20220807/epoch_582.h5\"\n",
    "    lstm_weight_name = \"_sp500LSTM\"\n",
    "elif lstm_weight_setting == \"tw50\":\n",
    "    lstm_weight_filepath = \"../exploration_other_time_series/models/20220816/tw50_20082017_epoch_246.h5\"\n",
    "    lstm_weight_name = \"_tw50LSTM\"\n",
    "elif lstm_weight_setting == \"tetuan_power\":\n",
    "    lstm_weight_filepath = \"../exploration_other_time_series/models/20220831/tetuan_power_res_epoch_597.h5\"\n",
    "    lstm_weight_name = \"_tetuan_powerLSTM\"\n",
    "\n",
    "# evaluation set\n",
    "eva_info = {\"paper_eva_1\": {\"portfolio\": ['PRGO', 'MRO', 'ADP', 'HCP', 'FITB', 'PEG', 'SYMC', 'EOG', 'MDT', 'NI'], \"file_name\": \"paper_eva_1_res\"},\n",
    "            \"paper_eva_2\": {\"portfolio\": ['STI', 'COP', 'MCD', 'AON', 'JBHT', 'DISH', 'GS', 'LRCX', 'CTXS', 'LEG'], \"file_name\": \"paper_eva_2_res\"},\n",
    "            \"paper_eva_3\": {\"portfolio\": ['TJX', 'EMN', 'JCI', 'C', 'BIIB', 'HOG', 'PX', 'PH', 'XEC', 'JEC'], \"file_name\": \"paper_eva_3_res\"},\n",
    "            \"paper_eva_4\": {\"portfolio\": ['ROP', 'AZO', 'URI', 'TROW', 'CMCSA', 'SLB', 'VZ', 'MAC', 'ADS', 'MCK'], \"file_name\": \"paper_eva_4_res\"},\n",
    "            \"paper_eva_5\": {\"portfolio\": ['RL', 'CVX', 'SRE', 'PFE', 'PCG', 'UTX', 'NTRS', 'INCY', 'COP', 'HRL'], \"file_name\": \"paper_eva_5_res\"},\n",
    "            \"445_all\": {\"portfolio\": portfolio_all, \"file_name\": \"sp500_20082017_all_res\"},\n",
    "            \"150_train\": {\"portfolio\": portfolio_train, \"file_name\": \"sp500_20082017_train_res\"},\n",
    "            \"295_other\": {\"portfolio\": portfolio_other, \"file_name\": \"sp500_20082017_test_res\"},\n",
    "           }\n",
    "\n",
    "\n",
    "eva_implement = \"295_other\"\n",
    "portfolio_implement = eva_info[eva_implement]['portfolio']\n",
    "output_file_name = eva_info[eva_implement]['file_name'] + time_period + lstm_weight_name\n",
    "fig_title = eva_implement + time_period + lstm_weight_name\n",
    "\n",
    "\n",
    "pd.to_datetime(stock_price_df['Date'], format='%Y-%m-%d')\n",
    "stock_price_df = stock_price_df.set_index(pd.DatetimeIndex(stock_price_df['Date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5303e902-4c2a-43c2-86a7-cbc68d096625",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 785 µs (started: 2022-09-10 02:23:16 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def gen_unseen_data_corr(portfolio: list, time_period:str = \"_test2\", ret_date: bool = False) -> \"pd.DataFrame, pd.Series | pd.DataFrame\":\n",
    "    tmp_corr = stock_price_df[portfolio[0]].rolling(window=100).corr(stock_price_df[portfolio[1]])\n",
    "    tmp_corr = tmp_corr.iloc[99::100]\n",
    "    if time_period == \"_test2\":\n",
    "        corr_series = tmp_corr[3:24] # correspond to test2_dataset of original paper\n",
    "    elif time_period == \"_test1\" :\n",
    "        corr_series = tmp_corr[2:23] # correspond to test1_dataset of original paper\n",
    "    elif time_period == \"_dev\":\n",
    "        corr_series = tmp_corr[1:22] # correspond to dev_dataset of original paper\n",
    "    elif time_period == \"_train\":\n",
    "        corr_series = tmp_corr[:21] # correspond to train_dataset of original papaer \n",
    "    unseen_data_df = pd.DataFrame(corr_series).reset_index().drop(['Date'], axis=1).T\n",
    "    if ret_date:\n",
    "        return unseen_data_df, corr_series\n",
    "    else:\n",
    "        return unseen_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e7765e10-2944-4621-b62f-6602a573387f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.301064</td>\n",
       "      <td>0.870396</td>\n",
       "      <td>0.895088</td>\n",
       "      <td>0.803884</td>\n",
       "      <td>0.763552</td>\n",
       "      <td>0.437946</td>\n",
       "      <td>0.671847</td>\n",
       "      <td>0.239142</td>\n",
       "      <td>0.703358</td>\n",
       "      <td>0.74433</td>\n",
       "      <td>...</td>\n",
       "      <td>0.396555</td>\n",
       "      <td>0.246283</td>\n",
       "      <td>0.103744</td>\n",
       "      <td>0.01833</td>\n",
       "      <td>0.188772</td>\n",
       "      <td>0.739464</td>\n",
       "      <td>-0.214199</td>\n",
       "      <td>0.285949</td>\n",
       "      <td>-0.352241</td>\n",
       "      <td>0.32908</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0         1         2         3         4         5         6   \\\n",
       "0  0.301064  0.870396  0.895088  0.803884  0.763552  0.437946  0.671847   \n",
       "\n",
       "         7         8        9   ...        11        12        13       14  \\\n",
       "0  0.239142  0.703358  0.74433  ...  0.396555  0.246283  0.103744  0.01833   \n",
       "\n",
       "         15        16        17        18        19       20  \n",
       "0  0.188772  0.739464 -0.214199  0.285949 -0.352241  0.32908  \n",
       "\n",
       "[1 rows x 21 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 9.5 ms (started: 2022-09-10 02:23:18 +00:00)\n"
     ]
    }
   ],
   "source": [
    "unseen_data_df = gen_unseen_data_corr(['RL', 'CVX'])\n",
    "unseen_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7976949-33c7-45c0-b6e9-c8cace53672a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c7ee34a-a8f6-4609-9e69-cfa66bc19100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.01 ms (started: 2022-09-09 07:14:40 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def arima_model(dataset: \"pd.DataFrame\", portfolio: list, overview: bool = False) -> (\"np.array\", \"pd.DataFrame\", str):\n",
    "    model_110 = ARIMA(order=(1, 1, 0), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_011 = ARIMA(order=(0, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_111 = ARIMA(order=(1, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_211 = ARIMA(order=(2, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_210 = ARIMA(order=(2, 1, 0), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    #model_330 = ARIMA(order=(3, 3, 0), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "\n",
    "    model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "    # model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111}\n",
    "\n",
    "    tested_models = []\n",
    "    arima_model = None\n",
    "    find_arima_model = False\n",
    "    residual = []\n",
    "    \n",
    "    # for s in np.array(dataset):\n",
    "    for _, corr_series in dataset.iterrows():\n",
    "        while not find_arima_model:\n",
    "            try:\n",
    "                for model_key in model_dict:\n",
    "                    if model_key not in tested_models:\n",
    "                        test_model = model_dict[model_key].fit(s)\n",
    "                        if arima_model is None:\n",
    "                            arima_model = test_model\n",
    "                            arima_model_name = model_key\n",
    "                        elif arima_model.aic() <= test_model.aic():\n",
    "                            pass\n",
    "                        else:\n",
    "                            arima_model = test_model\n",
    "                            arima_model_name = model_key\n",
    "                    tested_models.append(model_key)\n",
    "\n",
    "            except Exception:\n",
    "                if len(model_dict)-1 != 0:\n",
    "                    del model_dict[model_key]\n",
    "                else:\n",
    "                    print(f\"fatal error, {portfolio} doesn't have appropriate arima model\")\n",
    "                    break\n",
    "            else:\n",
    "                # model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111}\n",
    "                model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "                tested_models.clear()\n",
    "                find_arima_model = True\n",
    "        arima_pred = list(arima_model.predict_in_sample())\n",
    "        # arima_pred = list(arima_model.predict())\n",
    "        print(arima_model.predict())\n",
    "        # arima_pred = [np.mean(arima_pred[1:])] + arima_pred[1:]\n",
    "        # arima_pred = np.clip(np.array(arima_pred), -1, 1)\n",
    "\n",
    "        res = pd.Series(np.array(s) - arima_pred)\n",
    "        residual.append(np.array(res)[:20])\n",
    "        find_arima_model = False\n",
    "    residual = pd.DataFrame(residual)\n",
    "\n",
    "    if overview:\n",
    "        plt.plot(arima_pred, label=\"arima_pred\")\n",
    "        plt.plot(dataset.T, label=\"data\")\n",
    "        plt.plot(res, label=\"res\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return arima_pred, residual, arima_model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b896e4b4-e983-49af-b21c-c887106d85ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.03944619 -0.99245893  0.06952699]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([0.37458672, 0.26161747, 0.5265695 , 0.6099745 , 0.61901539,\n",
       "        0.60848646, 0.54060199, 0.5199191 , 0.44534054, 0.43460013,\n",
       "        0.42617751, 0.41628452, 0.37519024, 0.32579968, 0.27043928,\n",
       "        0.21412066, 0.17308318, 0.16712131, 0.10637046, 0.07643581,\n",
       "        0.01540436]),\n",
       " 0    -0.073523\n",
       " 1     0.608779\n",
       " 2     0.368518\n",
       " 3     0.193909\n",
       " 4     0.144537\n",
       " 5    -0.170540\n",
       " 6     0.131245\n",
       " 7    -0.280777\n",
       " 8     0.258017\n",
       " 9     0.309730\n",
       " 10    0.324435\n",
       " 11   -0.019729\n",
       " 12   -0.128908\n",
       " 13   -0.222055\n",
       " 14   -0.252110\n",
       " 15   -0.025348\n",
       " 16    0.566381\n",
       " 17   -0.381320\n",
       " 18    0.179579\n",
       " 19   -0.428676\n",
       " 20    0.313676\n",
       " dtype: float64,\n",
       " 'model_011',\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None,\n",
       " None)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 217 ms (started: 2022-09-10 02:26:36 +00:00)\n"
     ]
    }
   ],
   "source": [
    "portfolio = ['RL', 'CVX']\n",
    "unseen_data_corr_df, unseen_data_corr_series = gen_unseen_data_corr(portfolio, time_period=time_period, ret_date=True)\n",
    "# arima_pred, arima_model_name = arima_model(unseen_data_corr_df, portfolio, overview=True)\n",
    "tmp = arima_model(unseen_data_corr_df, portfolio, overview=False)\n",
    "tmp\n",
    "# display(arima_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd9fb1-b689-4e8d-9cda-677c4b0f97fb",
   "metadata": {},
   "source": [
    "# Temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "10a30da7-7a2f-4466-995c-b6f0ca7c8a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.08 ms (started: 2022-09-10 02:26:30 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def arima_model(dataset: \"pd.DataFrame\", portfolio: list, overview: bool = False) -> (\"np.array\", \"pd.DataFrame\", str):\n",
    "    model_110 = ARIMA(order=(1, 1, 0), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_011 = ARIMA(order=(0, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_111 = ARIMA(order=(1, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_211 = ARIMA(order=(2, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_210 = ARIMA(order=(2, 1, 0), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    #model_330 = ARIMA(order=(3, 3, 0), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "\n",
    "    model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "    # model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111}\n",
    "\n",
    "    tested_models = []\n",
    "    arima_model = None\n",
    "    arima_attr_list = [\"aic\", \"arparams\", \"aroots\", \"maparams\", \"maroots\", \"params\", \"pvalues\"]\n",
    "    arima_infos = dict(zip(arima_attr_list, [None]*len(arima_attr_list)))\n",
    "    find_arima_model = False\n",
    "    for _, corr_series in dataset.iterrows():\n",
    "        while not find_arima_model:\n",
    "            try:\n",
    "                for model_key in model_dict:\n",
    "                    if model_key not in tested_models:\n",
    "                        test_model = model_dict[model_key].fit(corr_series[:-1]) # only use first 20 corrletaion coefficient to fit ARIMA model\n",
    "                        if arima_model is None:\n",
    "                            arima_model = test_model\n",
    "                            arima_model_name = model_key\n",
    "                        elif arima_model.aic() <= test_model.aic():\n",
    "                            pass\n",
    "                        else:\n",
    "                            arima_model = test_model\n",
    "                            arima_model_name = model_key\n",
    "                    tested_models.append(model_key)\n",
    "            except Exception:\n",
    "                if len(model_dict)-1 != 0:\n",
    "                    del model_dict[model_key]\n",
    "                else:\n",
    "                    err_logger.error(f\"fatal error, {portfolio} doesn't have appropriate arima model\\n\", exc_info=True)\n",
    "                    break\n",
    "            else:\n",
    "                #model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210, \"model_330\": model_330}\n",
    "                model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "                tested_models.clear()\n",
    "                find_arima_model = True\n",
    "        try:\n",
    "            arima_pred = list(arima_model.predict(n_periods=1))\n",
    "        except Exception:\n",
    "            err_logger.error(f\"{portfolio} in {save_file_period} be predicted by {arima_model_name}(its aic:{arima_model.aic()}) getting error:\\n\", exc_info=True)\n",
    "        else:\n",
    "            arima_pred_in_sample = list(arima_model.predict_in_sample())\n",
    "            arima_pred_in_sample = [np.mean(arima_pred_in_sample[1:])] + arima_pred_in_sample[1:]\n",
    "            arima_output = arima_pred_in_sample + arima_pred\n",
    "            arima_output = np.clip(np.array(arima_output), -1, 1)\n",
    "            arima_resid = pd.Series(np.array(corr_series) - arima_output)\n",
    "            \n",
    "            print(getattr(arima_model, \"params\")())\n",
    "            \n",
    "            for attr in arima_infos.keys():\n",
    "                try:\n",
    "                    arima_infos[attr] = getattr(arima_model, attr)()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        finally:\n",
    "            find_arima_model = False\n",
    "    if overview:\n",
    "        plt.plot(arima_output, label=\"arima_pred\")\n",
    "        plt.plot(dataset.T, label=\"data\")\n",
    "        plt.plot(arima_resid, label=\"res\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return arima_output, arima_resid, arima_model_name, *[v for k, v in sorted(arima_info.items(),key=lambda x:x[0])] "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb455ad-4103-403d-a65f-ba8a13038295",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35591071-44de-4a02-8dfa-fb85bb7ae29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# class DoubleTanh(Activation):\n",
    "#     def __init__(self, activation, **kwargs):\n",
    "#         super(DoubleTanh, self).__init__(activation, **kwargs)\n",
    "#         self.__name__ = 'double_tanh'\n",
    "\n",
    "\n",
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) * 2)\n",
    "\n",
    "\n",
    "# get_custom_objects().update({'double_tanh': DoubleTanh(double_tanh)})\n",
    "\n",
    "# filepath = './stock_correlation_prediction/models/hybrid_LSTM_20220427/epoch262.h5'\n",
    "# custom_objects = {\"Double_Tanh\": DoubleTanh, }\n",
    "# with keras.utils.custom_object_scope(custom_objects):\n",
    "#     lstm_model = load_model(filepath)\n",
    "\n",
    "lstm_model = load_model(lstm_weight_filepath, custom_objects={'double_tanh':double_tanh})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65707dce-375f-4d15-84fd-e4edf9133cac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc480765-0e8f-4012-b4d6-40954dda0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stl_decompn(corr_series: \"pd.Series\", overview: bool = False) -> (float, float, float):\n",
    "    output_resid = 100000\n",
    "    output_trend = None\n",
    "    output_period = None\n",
    "    for p in range(2, 11):\n",
    "        decompose_result_mult = seasonal_decompose(corr_series, period=p)\n",
    "        resid_sum = np.abs(decompose_result_mult.resid).mean()\n",
    "        if output_resid > resid_sum:\n",
    "            output_resid = resid_sum\n",
    "            output_trend = decompose_result_mult.trend.dropna()\n",
    "            output_period = p\n",
    "    \n",
    "    reg = LinearRegression().fit(np.arange(len(output_trend)).reshape(-1, 1), output_trend)\n",
    "\n",
    "    if overview:\n",
    "        decompose_result_mult = seasonal_decompose(corr_series, period=output_period)\n",
    "        trend = decompose_result_mult.trend.dropna().reset_index(drop=True)\n",
    "        plt.figure(figsize=(7, 1))\n",
    "        plt.plot(trend)\n",
    "        plt.plot([0, len(trend)], [reg.intercept_, reg.intercept_+len(trend)*reg.coef_])\n",
    "        plt.title(\"trend & regression line\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        decompose_result_mult.plot()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return output_period, output_resid, output_trend.std(), reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f804c7-89e9-49bd-b23d-1a3dd2941030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unseen_data_corr_df, unseen_data_corr_series = gen_unseen_data_corr(['CNP', 'RSG'], ret_date=True)\n",
    "stl_decompn(unseen_data_corr_series, overview=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113b850-5406-481e-bcbd-a6831edb95c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_list = []\n",
    "unseen_data_corr_df_concat = pd.DataFrame(columns=list(range(21))+['portfolio'])\n",
    "unseen_data_arima_resid_concat = pd.DataFrame(columns=list(range(20))+['portfolio'])\n",
    "count = 0\n",
    "for portfolio in tqdm(combinations(portfolio_implement, 2)):\n",
    "    unseen_data_corr_df, unseen_data_corr_series = gen_unseen_data_corr(portfolio, time_period=time_period, ret_date=True)\n",
    "    arima_pred, residual, arima_model_name = arima_model(unseen_data_corr_df, portfolio)\n",
    "    unseen_res = residual.values.reshape((-1, 20, 1))\n",
    "    lstm_pred = lstm_model.predict(unseen_res)\n",
    "    season_period, stl_resid, stl_trend_std, coef_reg_trend = stl_decompn(unseen_data_corr_series)\n",
    "    portfolio_res_dic = {\"portfolio\": f\"{portfolio[0]} & {portfolio[1]}\",\n",
    "                         \"corr_ser_mean\": unseen_data_corr_series.mean(),\n",
    "                         \"corr_ser_std\": unseen_data_corr_series.std(),\n",
    "                         \"corr_season_period\": season_period,\n",
    "                         \"corr_stl_resid\": stl_resid,\n",
    "                         \"corr_stl_trend_std\": stl_trend_std,\n",
    "                         \"corr_trend_coef\": coef_reg_trend,\n",
    "                         \"arima_model\": arima_model_name,\n",
    "                         \"lstm_pred\": lstm_pred[0][0],\n",
    "                         \"arima_pred\": arima_pred[-1],\n",
    "                         \"hybrid_model_pred\": arima_pred[-1]+lstm_pred[0][0],\n",
    "                         \"ground_truth\": unseen_data_corr_df.iloc[0, -1],\n",
    "                         \"arima_err\": unseen_data_corr_df.iloc[0, -1] - arima_pred[-1],\n",
    "                         \"error\": (unseen_data_corr_df.iloc[0, -1] - (arima_pred[-1]+lstm_pred[0][0])),\n",
    "                         \"absolute_err\": math.copysign((unseen_data_corr_df.iloc[0, -1] - (arima_pred[-1]+lstm_pred[0][0])), 1), \n",
    "                         \"lstm_compensation_dir\": np.sign(unseen_data_corr_df.iloc[0, -1] - arima_pred[-1])*np.sign(lstm_pred[0][0])}\n",
    "\n",
    "    res_list.append(portfolio_res_dic)\n",
    "    unseen_data_corr_df['portfolio'] = f\"{portfolio[0]} & {portfolio[1]}\"\n",
    "    unseen_data_corr_df_concat = pd.concat([unseen_data_corr_df_concat, unseen_data_corr_df]) \n",
    "    residual['portfolio'] = f\"{portfolio[0]} & {portfolio[1]}\"\n",
    "    unseen_data_arima_resid_concat = pd.concat([unseen_data_arima_resid_concat, residual]) \n",
    "\n",
    "if save_raw_corr_data:\n",
    "    unseen_data_corr_df_concat = unseen_data_corr_df_concat.set_index('portfolio')\n",
    "    unseen_data_corr_df_concat.to_csv(f\"./stock_correlation_prediction/use_hybrid_model/{output_file_name}_raw_corr.csv\", index=True)\n",
    "    \n",
    "if save_arima_resid_data:\n",
    "    unseen_data_arima_resid_concat = unseen_data_arima_resid_concat.set_index('portfolio')\n",
    "    unseen_data_arima_resid_concat.to_csv(f\"./stock_correlation_prediction/use_hybrid_model/{output_file_name}_arima_resid.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810010d8-acca-45cb-b21b-2871ce2e5607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(res_list)\n",
    "res_df.to_csv(f\"./stock_correlation_prediction/use_hybrid_model/{output_file_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8daca-b55f-4a00-b162-4968bf696c50",
   "metadata": {},
   "source": [
    "# Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84895d7-ae0e-4cf9-9ecc-2adf76c38462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(f\"./stock_correlation_prediction/use_hybrid_model/{output_file_name}.csv\")\n",
    "display(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89180e3d-ee6c-4b6d-a05e-ea386f3f272f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def res_df_postprocess(target_df: pd.core.frame.DataFrame) -> None:\n",
    "    target_df['arima_pred_dir'] = np.sign(target_df['ground_truth'] * target_df['arima_pred'])\n",
    "    target_df['arima_err'] = target_df['ground_truth'] - target_df['arima_pred']\n",
    "    quantile_mask = np.logical_and(res_df['error'] < np.quantile(res_df['error'], 0.75), res_df['error'] > np.quantile(res_df['error'], 0.25)).tolist()\n",
    "    display(np.quantile(res_df['error'], 0.75), np.quantile(res_df['error'], 0.25))\n",
    "    target_df['high_pred_performance'] = quantile_mask\n",
    "    target_df['portfolio[0]'] = target_df.apply(lambda row:row['portfolio'].split(\" & \")[0], axis=1)\n",
    "    target_df['portfolio[1]'] = target_df.apply(lambda row:row['portfolio'].split(\" & \")[1], axis=1)\n",
    "\n",
    "\n",
    "res_df_postprocess(res_df)\n",
    "display(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7c64a-f3c7-4b5d-93bc-737c4ae4bf67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_exploration(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=7, ncols=2, sharex=False, sharey=False, dpi=100)\n",
    "    s0 = axes[0, 0]\n",
    "    s0.set_title(\"ABS_err violin\")\n",
    "    sns.violinplot(y=target_df[\"absolute_err\"], ax=s0)\n",
    "    s1 = axes[0, 1]\n",
    "    s1.set_title(\"Err violin\")\n",
    "    sns.violinplot(y=target_df[\"error\"], ax=s1)\n",
    "    s2 = axes[1, 0]\n",
    "    s2.set_title(\"ABS_err hist\")\n",
    "    target_df['absolute_err'].hist(bins=[b/10 for b in range(11)], ax=s2)\n",
    "    s3 = axes[1, 1]\n",
    "    s3.set_title(\"Err hist\")\n",
    "    target_df['error'].hist(bins=[b/10 for b in range(-10, 11)], ax=s3)\n",
    "    s4 = axes[2, 0]\n",
    "    s4.set_title(\"LSTM_compensation_dir count\")\n",
    "    sns.countplot(x=\"lstm_compensation_dir\", data=target_df, ax=s4)\n",
    "    s5 = axes[2, 1]\n",
    "    s5.set_title(\"LSTM_compensation_dir count groupby ARIMA_pred_dir\")\n",
    "    df_gb = target_df.groupby(['arima_pred_dir', 'lstm_compensation_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s5)\n",
    "    s6 = axes[3, 0]\n",
    "    s6.set_title(\"ARIMA_model prediction Err violin group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"arima_err\"], ax=s6)\n",
    "    s8 = axes[4, 0]\n",
    "    s8.set_title(\"ARIMA_model prediction magnitude group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"arima_pred\"], ax=s8)\n",
    "    s9 = axes[4, 1]\n",
    "    s9.set_title(\"LSTM compensation magnitude group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"lstm_pred\"], ax=s9)\n",
    "    s10 = axes[5, 0]\n",
    "    s10.set_title(\"Correlation magnitude in last period group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"ground_truth\"], ax=s10)\n",
    "    s11 = axes[5, 1]\n",
    "    s11.set_title(\"Hybrid Err violin group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"error\"], ax=s11)\n",
    "    s12 = axes[6,0]\n",
    "    s12.set_title(\"LSTM_compensation_dir pie with wrong ARIMA_pred_dir\")\n",
    "    df_gb.loc[df_gb.index==-1, :].squeeze().plot(kind=\"pie\", autopct='%1.1f%%', ax=s12)\n",
    "    s13 = axes[6,1]\n",
    "    s13.set_title(\"LSTM_compensation_dir pie with correct ARIMA_pred_dir\")\n",
    "    df_gb.loc[df_gb.index==1, :].squeeze().plot(kind=\"pie\", autopct='%1.1f%%', ax=s13)\n",
    "    \n",
    "    fig.suptitle(f\"{title}_basic_exploration\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./stock_correlation_prediction/use_hybrid_model/hybrid_prediction_analysis_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0699ee9-76fc-4804-8981-71997148f358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_exploration_pred_perform(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=6, ncols=2, sharex=False, sharey=False, dpi=100)\n",
    "    s1 = axes[0, 0]\n",
    "    s1.set_title(\"LSTM_compensation_dir count groupby prediction performance\")\n",
    "    df_gb = target_df.groupby(['high_pred_performance', 'lstm_compensation_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s1)\n",
    "    s2 = axes[0, 1]\n",
    "    s2.set_title(\"ARIMA_model prediction magnitude group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"arima_pred\"], ax=s2)\n",
    "    s3 = axes[1, 0]\n",
    "    s3.set_title(\"LSTM compensation magnitude group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"lstm_pred\"], ax=s3)\n",
    "    s4 = axes[1, 1]\n",
    "    s4.set_title(\"Correlation magnitude in last period group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"ground_truth\"], ax=s4)\n",
    "    s5 = axes[2, 0]\n",
    "    s5.set_title(\"Correlation series mean groupby prediction performance\")\n",
    "    sns.violinplot(x=target_df['high_pred_performance'], y=target_df[\"corr_ser_mean\"], ax=s5)\n",
    "    s6 = axes[2, 1]\n",
    "    s6.set_title(\"Correlation series std groupby prediction performance\")\n",
    "    sns.violinplot(x=target_df['high_pred_performance'], y=target_df[\"corr_ser_std\"], ax=s6)\n",
    "    s7 = axes[3, 0]\n",
    "    s7.set_title(\"Correlation series stl_period group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_season_period\"], ax=s7)\n",
    "    s8 = axes[3, 1]\n",
    "    s8.set_title(\"Correlation series stl_residual group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_resid\"], ax=s8)\n",
    "    s9 = axes[4, 0]\n",
    "    s9.set_title(\"Correlation series stl_trend_std group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_trend_std\"], ax=s9)\n",
    "    s10 = axes[4, 1]\n",
    "    s10.set_title(\"Correlation series stl_trend_coef group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_trend_coef\"], ax=s10)\n",
    "    s11 = axes[5, 0]\n",
    "    s11.set_title(\"ARIMA_pred_dir count groupby prediction performance\")\n",
    "    df_gb = target_df.groupby(['high_pred_performance', 'arima_pred_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s11)\n",
    "\n",
    "    fig.suptitle(F\"{title}_groupby prediction\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./stock_correlation_prediction/use_hybrid_model/hybrid_prediction_analysis_groupby_pred_perform_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3dc54e-6c33-43a4-adb0-a15ffced28ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stock_freq(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    stocks_show_freq = target_df.loc[target_df['high_pred_performance'] == True, ['portfolio[0]','portfolio[1]']].stack().value_counts().to_dict()\n",
    "    plt.figure(figsize=(80, 10), dpi=100)\n",
    "    plt.bar(range(len(stocks_show_freq)), list(stocks_show_freq.values()))\n",
    "    plt.xticks(range(len(stocks_show_freq)), list(stocks_show_freq.keys()), rotation=60)\n",
    "    plt.title(F\"{title}_stock appearence frequence\")\n",
    "    plt.savefig(f\"./stock_correlation_prediction/use_hybrid_model/stock_appearence_frequence_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c94193-1825-45f3-92fb-5d252cdae48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f\"mse :{(res_df['error']**2).mean()}\",\n",
    "        f\"std of square_err :{(res_df['error']**2).std()}\",\n",
    "        f\"rmse :{np.sqrt((res_df['error']**2).mean())}\",\n",
    "        f\"mae : {res_df['absolute_err'].mean()}\",\n",
    "        f\"std of abs_err: {res_df['absolute_err'].std()}\")\n",
    "\n",
    "# display(f\"sklearn mse: {mean_squared_error(res_df['ground_truth'], res_df['hybrid_model_pred'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dda3ba-5e75-4610-8cf7-2de853713ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_exploration(res_df, fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c7e89-18db-498a-b3bd-2ad63a185935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_exploration_pred_perform(res_df, fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751a5ad-6e92-40a3-915a-109ed10d8c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_stock_freq(res_df, fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8151bfc-9a82-4854-a759-e422f3d579ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(res_df.loc[res_df['high_pred_performance']==True, ['portfolio[0]','portfolio[1]']].stack()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f86314-5b10-4b56-920a-73b53a2c8040",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
