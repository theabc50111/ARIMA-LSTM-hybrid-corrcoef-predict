{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41660f84-97c9-4162-a708-f601b3b7a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.92 s (started: 2022-09-10 17:35:21 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression \n",
    "from pmdarima.arima import ARIMA, auto_arima\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import warnings\n",
    "import logging\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "err_log_handler = logging.FileHandler(filename=\"./results/arima_train_err_log.txt\", mode='a')        \n",
    "err_logger = logging.getLogger(\"arima_train_err\")\n",
    "err_logger.addHandler(err_log_handler)\n",
    "\n",
    "\n",
    "%load_ext pycodestyle_magic\n",
    "%pycodestyle_on --ignore E501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a012b1-6376-486e-b5dd-93c76929fc3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4848bc85-9efc-4bc5-a63a-066bbc4d16ca",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pycodestyle:5:80: E501 line too long (84 > 79 characters)\n",
      "INFO:pycodestyle:6:36: E114 indentation is not a multiple of 4 (comment)\n",
      "INFO:pycodestyle:6:36: E116 unexpected indentation (comment)\n",
      "INFO:pycodestyle:8:80: E501 line too long (89 > 79 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 578 µs (started: 2022-09-10 17:35:24 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# setting of output files\n",
    "save_raw_corr_data = True\n",
    "save_arima_resid_data = True\n",
    "# data implement setting\n",
    "data_implement = \"sp500_20082017\"  # tw50|sp500_20082017|sp500_19972007|tetuan_power\n",
    "                                   # |paper_eva_1|paper_eva_2|paper_eva_3|paper_eva_4|paper_eva_5\n",
    "# lstm weight setting\n",
    "lstm_weight_setting = \"sp500_20082017\"  # tw50|sp500_20082017|sp500_19972007|tetuan_power\n",
    "# evaluation set setting\n",
    "items_setting = \"test\"  # test|all\n",
    "# data time period setting\n",
    "time_period = \"_test2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b431c52f-e408-4b59-b512-ce32d8607d04",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:===== len(train_set): 150, len(all_set): 445, len(test_set): 296 =====\n",
      "INFO:root:===== len(evaluation set): 296 =====\n",
      "INFO:root:===== LSTM weight:_sp500_20082017LSTM =====\n",
      "INFO:root:===== file_name basis:sp500_20082017_res_test_test2_sp500_20082017LSTM, fig_title basis:sp500_20082017_test_test2_sp500_20082017LSTM =====\n",
      "INFO:pycodestyle:4:80: E501 line too long (507 > 79 characters)\n",
      "INFO:pycodestyle:7:80: E501 line too long (769 > 79 characters)\n",
      "INFO:pycodestyle:8:80: E501 line too long (117 > 79 characters)\n",
      "INFO:pycodestyle:8:118: W291 trailing whitespace\n",
      "INFO:pycodestyle:10:80: E501 line too long (1071 > 79 characters)\n",
      "INFO:pycodestyle:13:80: E501 line too long (183 > 79 characters)\n",
      "INFO:pycodestyle:19:80: E501 line too long (123 > 79 characters)\n",
      "INFO:pycodestyle:28:80: E501 line too long (150 > 79 characters)\n",
      "INFO:pycodestyle:29:80: E501 line too long (151 > 79 characters)\n",
      "INFO:pycodestyle:30:80: E501 line too long (145 > 79 characters)\n",
      "INFO:pycodestyle:31:80: E501 line too long (150 > 79 characters)\n",
      "INFO:pycodestyle:32:80: E501 line too long (149 > 79 characters)\n",
      "INFO:pycodestyle:33:80: E501 line too long (83 > 79 characters)\n",
      "INFO:pycodestyle:34:80: E501 line too long (94 > 79 characters)\n",
      "INFO:pycodestyle:35:80: E501 line too long (94 > 79 characters)\n",
      "INFO:pycodestyle:36:80: E501 line too long (91 > 79 characters)\n",
      "INFO:pycodestyle:41:80: E501 line too long (105 > 79 characters)\n",
      "INFO:pycodestyle:51:80: E501 line too long (114 > 79 characters)\n",
      "INFO:pycodestyle:53:80: E501 line too long (92 > 79 characters)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 117 ms (started: 2022-09-10 17:40:27 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# data loading & implement setting\n",
    "dataset_path = Path(\"./dataset/\")\n",
    "if data_implement == \"tw50\":\n",
    "    file_name = Path(\"tw50_hold_20082018_adj_close_pre.csv\")\n",
    "    train_set = ['萬海_adj_close', '豐泰_adj_close', '友達_adj_close', '欣興_adj_close', '台塑化_adj_close', '和泰車_adj_close', '元大金_adj_close', '南電_adj_close', '台塑_adj_close', '統一超_adj_close', '台泥_adj_close', '瑞昱_adj_close', '彰銀_adj_close', '富邦金_adj_close', '研華_adj_close', '中鋼_adj_close', '鴻海_adj_close', '台新金_adj_close', '遠傳_adj_close', '南亞_adj_close', '台達電_adj_close', '台灣大_adj_close', '台化_adj_close', '聯詠_adj_close', '廣達_adj_close', '聯發科_adj_close', '台積電_adj_close', '統一_adj_close', '中信金_adj_close', '長榮_adj_close']\n",
    "elif data_implement == \"sp500_19972007\":\n",
    "    file_name = Path(\"sp500_hold_19972007_adj_close_pre.csv\")\n",
    "    train_set = ['PXD', 'WAT', 'LH', 'AMGN', 'AOS', 'EFX', 'NEM', 'CTAS', 'MAT', 'VLO', 'APH', 'ADM', 'MLM', 'BK', 'NOV', 'BDX', 'RRC', 'IVZ', 'ED', 'SBUX', 'CI', 'ZION', 'COO', 'FDX', 'GLW', 'GPC', 'HPQ', 'ADI', 'AMG', 'MTB', 'YUM', 'SYK', 'KMX', 'AME', 'BMY', 'KMB', 'JPM', 'AET', 'DLTR', 'MGM', 'FL', 'HD', 'CLX', 'OKE', 'WMB', 'IFF', 'CMS', 'MMC', 'REG', 'ES', 'ITW', 'VRTX', 'QCOM', 'MSI', 'NKTR', 'AMAT', 'BWA', 'ESRX', 'TXT', 'VNO', 'WDC', 'PVH', 'NOC', 'PCAR', 'NSC', 'PHM', 'LUV', 'HUM', 'SPG', 'SJM', 'ABT', 'ALK', 'TAP', 'CAT', 'TMO', 'AES', 'MRK', 'RMD', 'MKC', 'HIG', 'DE', 'ATVI', 'O', 'UNM', 'VMC', 'CMA', 'RHI', 'RE', 'FMC', 'MU', 'CB', 'LNT', 'GE', 'SNA', 'LLY', 'LEN', 'MAA', 'OMC', 'F', 'APA', 'CDNS', 'SLG', 'HP', 'SHW', 'AFL', 'STT', 'PAYX', 'AIG']\n",
    "elif data_implement in [\"sp500_20082017\", \"paper_eva_1\", \"paper_eva_2\", \"paper_eva_3\", \"paper_eva_4\", \"paper_eva_5\"]:    \n",
    "    file_name = Path(\"stock08_price.csv\")\n",
    "    train_set = ['CELG', 'PXD', 'WAT', 'LH', 'AMGN', 'AOS', 'EFX', 'CRM', 'NEM', 'JNPR', 'LB', 'CTAS', 'MAT', 'MDLZ', 'VLO', 'APH', 'ADM', 'MLM', 'BK', 'NOV', 'BDX', 'RRC', 'IVZ', 'ED', 'SBUX', 'GRMN', 'CI', 'ZION', 'COO', 'TIF', 'RHT', 'FDX', 'LLL', 'GLW', 'GPN', 'IPGP', 'GPC', 'HPQ', 'ADI', 'AMG', 'MTB', 'YUM', 'SYK', 'KMX', 'AME', 'AAP', 'DAL', 'A', 'MON', 'BRK', 'BMY', 'KMB', 'JPM', 'CCI', 'AET', 'DLTR', 'MGM', 'FL', 'HD', 'CLX', 'OKE', 'UPS', 'WMB', 'IFF', 'CMS', 'ARNC', 'VIAB', 'MMC', 'REG', 'ES', 'ITW', 'NDAQ', 'AIZ', 'VRTX', 'CTL', 'QCOM', 'MSI', 'NKTR', 'AMAT', 'BWA', 'ESRX', 'TXT', 'EXR', 'VNO', 'BBT', 'WDC', 'UAL', 'PVH', 'NOC', 'PCAR', 'NSC', 'UAA', 'FFIV', 'PHM', 'LUV', 'HUM', 'SPG', 'SJM', 'ABT', 'CMG', 'ALK', 'ULTA', 'TMK', 'TAP', 'SCG', 'CAT', 'TMO', 'AES', 'MRK', 'RMD', 'MKC', 'WU', 'ACN', 'HIG', 'TEL', 'DE', 'ATVI', 'O', 'UNM', 'VMC', 'ETFC', 'CMA', 'NRG', 'RHI', 'RE', 'FMC', 'MU', 'CB', 'LNT', 'GE', 'CBS', 'ALGN', 'SNA', 'LLY', 'LEN', 'MAA', 'OMC', 'F', 'APA', 'CDNS', 'SLG', 'HP', 'XLNX', 'SHW', 'AFL', 'STT', 'PAYX', 'AIG', 'FOX', 'MA']\n",
    "elif data_implement == \"tetuan_power\":\n",
    "    file_name = Path(\"Tetuan City power consumption_pre.csv\")\n",
    "    train_set = [\"Temperature\", \"Humidity\", \"Wind Speed\", \"general diffuse flows\", \"diffuse flows\", \"Zone 1 Power Consumption\", \"Zone 2 Power Consumption\", \"Zone 3 Power Consumption\"]\n",
    "\n",
    "dataset_df = pd.read_csv(dataset_path/file_name)\n",
    "dataset_df = dataset_df.set_index('Date')\n",
    "all_set = list(dataset_df.columns.values[1:])  # all data\n",
    "test_set = [p for p in all_set if p not in train_set]  # all data - train data\n",
    "logging.info(f\"===== len(train_set): {len(train_set)}, len(all_set): {len(all_set)}, len(test_set): {len(test_set)} =====\")\n",
    "\n",
    "# evaluation set setting\n",
    "if items_setting == \"all\":\n",
    "    items_set = all_set\n",
    "    output_set_name = \"_all\"\n",
    "elif items_setting == \"test\":\n",
    "    items_set = test_set\n",
    "    output_set_name = \"_test\"\n",
    "\n",
    "evaluation_info = {\"paper_eva_1\": {\"items\": ['PRGO', 'MRO', 'ADP', 'HCP', 'FITB', 'PEG', 'SYMC', 'EOG', 'MDT', 'NI'], \"file_name\": \"paper_eva_1_res\"},\n",
    "                   \"paper_eva_2\": {\"items\": ['STI', 'COP', 'MCD', 'AON', 'JBHT', 'DISH', 'GS', 'LRCX', 'CTXS', 'LEG'], \"file_name\": \"paper_eva_2_res\"},\n",
    "                   \"paper_eva_3\": {\"items\": ['TJX', 'EMN', 'JCI', 'C', 'BIIB', 'HOG', 'PX', 'PH', 'XEC', 'JEC'], \"file_name\": \"paper_eva_3_res\"},\n",
    "                   \"paper_eva_4\": {\"items\": ['ROP', 'AZO', 'URI', 'TROW', 'CMCSA', 'SLB', 'VZ', 'MAC', 'ADS', 'MCK'], \"file_name\": \"paper_eva_4_res\"},\n",
    "                   \"paper_eva_5\": {\"items\": ['RL', 'CVX', 'SRE', 'PFE', 'PCG', 'UTX', 'NTRS', 'INCY', 'COP', 'HRL'], \"file_name\": \"paper_eva_5_res\"},\n",
    "                   \"tw50\": {\"items\": items_set, \"file_name\": f\"tw50_20082017_res\"},\n",
    "                   \"sp500_19972007\": {\"items\": items_set, \"file_name\": f\"sp500_19972007_res\"},\n",
    "                   \"sp500_20082017\": {\"items\": items_set, \"file_name\": f\"sp500_20082017_res\"},\n",
    "                   \"tetuan_power\": {\"items\": items_set, \"file_name\":  f\"tetuan_power_res\"}}\n",
    "items_implement = evaluation_info[data_implement]['items']\n",
    "logging.info(f\"===== len(evaluation set): {len(items_implement)} =====\")\n",
    "\n",
    "# lstm weight setting\n",
    "if lstm_weight_setting == \"sp500_20082017\":\n",
    "    lstm_weight_filepath = \"../rebuild_hybrid_model/models/20220909/sp500_20082017_train_res_epoch_43.h5\"\n",
    "    lstm_weight_name = \"_sp500_20082017LSTM\"\n",
    "elif lstm_weight_setting == \"tw50\":\n",
    "    lstm_weight_filepath = \"./models/20220816/tw50_20082017_epoch_246.h5\"\n",
    "    lstm_weight_name = \"_tw50LSTM\"\n",
    "elif lstm_weight_setting == \"tetuan_power\":\n",
    "    lstm_weight_filepath = \"./models/20220831/tetuan_power_res_epoch_597.h5\"\n",
    "    lstm_weight_name = \"_tetuan_powerLSTM\"\n",
    "logging.info(f\"===== LSTM weight:{lstm_weight_name} =====\")\n",
    "\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = evaluation_info[data_implement]['file_name'] + output_set_name + time_period + lstm_weight_name\n",
    "fig_title = data_implement + output_set_name + time_period + lstm_weight_name\n",
    "logging.info(f\"===== file_name basis:{output_file_name}, fig_title basis:{fig_title} =====\")\n",
    "\n",
    "# display(dataset_df)\n",
    "# display(all_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303e902-4c2a-43c2-86a7-cbc68d096625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_unseen_data_corr(items: list, time_period:str = \"_test2\", ret_date: bool = False) -> \"pd.DataFrame, pd.Series | pd.DataFrame\":\n",
    "    tmp_corr = dataset_df[items[0]].rolling(window=100).corr(dataset_df[items[1]])\n",
    "    tmp_corr = tmp_corr.iloc[99::100]\n",
    "    if time_period == \"_test2\":\n",
    "        corr_series = tmp_corr[3:24] # correspond to test2_dataset of original paper\n",
    "    elif time_period == \"_test1\" :\n",
    "        corr_series = tmp_corr[2:23] # correspond to test1_dataset of original paper\n",
    "    elif time_period == \"_dev\":\n",
    "        corr_series = tmp_corr[1:22] # correspond to dev_dataset of original paper\n",
    "    elif time_period == \"_train\":\n",
    "        corr_series = tmp_corr[:21] # correspond to train_dataset of original papaer \n",
    "    unseen_data_df = pd.DataFrame(corr_series).reset_index().drop(['Date'], axis=1).T\n",
    "    if ret_date:\n",
    "        return unseen_data_df, corr_series\n",
    "    else:\n",
    "        return unseen_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a127e-cfe8-4035-b507-43406374dcf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(gen_unseen_data_corr([items_implement[0], items_implement[1]], time_period=\"_test2\"))\n",
    "display(gen_unseen_data_corr([items_implement[0], items_implement[1]], time_period=\"_test1\"))\n",
    "display(gen_unseen_data_corr([items_implement[0], items_implement[1]], time_period=\"_dev\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7976949-33c7-45c0-b6e9-c8cace53672a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ee34a-a8f6-4609-9e69-cfa66bc19100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def arima_model(dataset: \"pd.DataFrame\", portfolio: list, overview: bool = False) -> (\"np.array\", \"pd.DataFrame\", str):\n",
    "    model_110 = ARIMA(order=(1, 1, 0), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_011 = ARIMA(order=(0, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_111 = ARIMA(order=(1, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_211 = ARIMA(order=(2, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    model_210 = ARIMA(order=(2, 1, 0), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "    # model_330 = ARIMA(order=(3, 3, 0), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "\n",
    "    model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "    # model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111}\n",
    "\n",
    "    tested_models = []\n",
    "    arima_model = None\n",
    "    arima_attr_list = [\"aic\", \"arparams\", \"aroots\", \"maparams\", \"maroots\", \"params\", \"pvalues\"]\n",
    "    arima_infos = dict(zip(arima_attr_list, [None]*len(arima_attr_list)))\n",
    "    find_arima_model = False\n",
    "    for _, corr_series in dataset.iterrows():\n",
    "        while not find_arima_model:\n",
    "            try:\n",
    "                for model_key in model_dict:\n",
    "                    if model_key not in tested_models:\n",
    "                        test_model = model_dict[model_key].fit(corr_series[:-1])  # only use first 20 corrletaion coefficient to fit ARIMA model\n",
    "                        if arima_model is None:\n",
    "                            arima_model = test_model\n",
    "                            arima_model_name = model_key\n",
    "                        elif arima_model.aic() <= test_model.aic():\n",
    "                            pass\n",
    "                        else:\n",
    "                            arima_model = test_model\n",
    "                            arima_model_name = model_key\n",
    "                    tested_models.append(model_key)\n",
    "            except Exception:\n",
    "                if len(model_dict)-1 != 0:\n",
    "                    del model_dict[model_key]\n",
    "                else:\n",
    "                    err_logger.error(f\"fatal error, {portfolio} doesn't have appropriate arima model\\n\", exc_info=True)\n",
    "                    raise NotImplementedError(f\"fatal error, {portfolio} doesn't have appropriate arima model\\n\")\n",
    "            else:\n",
    "                # model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210, \"model_330\": model_330}\n",
    "                model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "                tested_models.clear()\n",
    "                find_arima_model = True\n",
    "        try:\n",
    "            arima_pred = list(arima_model.predict(n_periods=1))\n",
    "        except Exception:\n",
    "            err_logger.error(f\"{portfolio} in {time_period} be predicted by {arima_model_name}(its aic:{arima_model.aic()}) getting error:\\n\", exc_info=True)\n",
    "            raise NotImplementedError(f\"{portfolio} in {time_period} be predicted by {arima_model_name}(its aic:{arima_model.aic()}) getting error\\n\")\n",
    "        else:\n",
    "            arima_pred_in_sample = list(arima_model.predict_in_sample())\n",
    "            arima_pred_in_sample = [np.mean(arima_pred_in_sample[1:])] + arima_pred_in_sample[1:]\n",
    "            arima_output = arima_pred_in_sample + arima_pred\n",
    "            arima_output = np.clip(np.array(arima_output), -1, 1)\n",
    "\n",
    "            arima_resid = pd.Series(np.array(corr_series) - arima_output).iloc[:-1]\n",
    "\n",
    "            for attr in arima_infos.keys():\n",
    "                try:\n",
    "                    arima_infos[attr] = getattr(arima_model, attr)()\n",
    "                except AttributeError:\n",
    "                    pass\n",
    "        finally:\n",
    "            find_arima_model = False\n",
    "    if overview:\n",
    "        plt.plot(arima_output, label=\"arima_pred\")\n",
    "        plt.plot(dataset.T, label=\"data\")\n",
    "        plt.plot(arima_resid, label=\"res\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return arima_output, arima_resid, arima_model_name, *[v for k, v in sorted(arima_infos.items(), key=lambda x:x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50ab82-3053-41e2-b20c-d2a4bcb0fe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_data_corr_df = gen_unseen_data_corr([items_implement[0], items_implement[1]], time_period=\"_test2\")\n",
    "arima_pred, residual, arima_model_name, arima_aic, arima_arparams, arima_aroots, arima_maparams, arima_maroots, arima_params, arima_pvalues = arima_model(unseen_data_corr_df, [items_implement[0], items_implement[1]])\n",
    "display(len(arima_pred), len(residual))\n",
    "display(arima_pred, residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb455ad-4103-403d-a65f-ba8a13038295",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35591071-44de-4a02-8dfa-fb85bb7ae29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) * 2)\n",
    "\n",
    "\n",
    "lstm_model = load_model(lstm_weight_filepath, custom_objects={'double_tanh':double_tanh})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65707dce-375f-4d15-84fd-e4edf9133cac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc480765-0e8f-4012-b4d6-40954dda0a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stl_decompn(corr_series: \"pd.Series\", overview: bool = False) -> (float, float, float):\n",
    "    output_resid = 100000\n",
    "    output_trend = None\n",
    "    output_period = None\n",
    "    for p in range(2, 11):\n",
    "        decompose_result_mult = seasonal_decompose(corr_series, period=p)\n",
    "        resid_sum = np.abs(decompose_result_mult.resid).mean()\n",
    "        if output_resid > resid_sum:\n",
    "            output_resid = resid_sum\n",
    "            output_trend = decompose_result_mult.trend.dropna()\n",
    "            output_period = p\n",
    "    \n",
    "    reg = LinearRegression().fit(np.arange(len(output_trend)).reshape(-1, 1), output_trend)\n",
    "\n",
    "    if overview:\n",
    "        decompose_result_mult = seasonal_decompose(corr_series, period=output_period)\n",
    "        trend = decompose_result_mult.trend.dropna().reset_index(drop=True)\n",
    "        plt.figure(figsize=(7, 1))\n",
    "        plt.plot(trend)\n",
    "        plt.plot([0, len(trend)], [reg.intercept_, reg.intercept_+len(trend)*reg.coef_])\n",
    "        plt.title(\"trend & regression line\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "        decompose_result_mult.plot()\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    return output_period, output_resid, output_trend.std(), reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f804c7-89e9-49bd-b23d-1a3dd2941030",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "unseen_data_corr_df, unseen_data_corr_series = gen_unseen_data_corr([items_implement[0], items_implement[1]], ret_date=True)\n",
    "stl_decompn(unseen_data_corr_series, overview=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113b850-5406-481e-bcbd-a6831edb95c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_list = []\n",
    "unseen_data_corr_df_concat = pd.DataFrame(columns=list(range(21))+['items'])\n",
    "unseen_data_arima_resid_concat = pd.DataFrame(columns=list(range(20))+['items'])\n",
    "count = 0\n",
    "for items in tqdm(combinations(items_implement, 2)):\n",
    "    unseen_data_corr_df, unseen_data_corr_series = gen_unseen_data_corr(items, time_period=time_period, ret_date=True)\n",
    "    try:\n",
    "        arima_pred, residual, arima_model_name, arima_aic, arima_arparams, arima_aroots, arima_maparams, arima_maroots, arima_params, arima_pvalues = arima_model(unseen_data_corr_df, items)\n",
    "    except NotImplementedError:\n",
    "        continue\n",
    "    else:\n",
    "        unseen_res = residual.values.reshape((-1, 20, 1))\n",
    "        lstm_pred = lstm_model.predict(unseen_res)\n",
    "        season_period, stl_resid, stl_trend_std, coef_reg_trend = stl_decompn(unseen_data_corr_series)\n",
    "        items_res_dic = {\"items\": f\"{items[0]} & {items[1]}\",\n",
    "                         \"corr_ser_mean\": unseen_data_corr_series.mean(),\n",
    "                         \"corr_ser_std\": unseen_data_corr_series.std(),\n",
    "                         \"corr_season_period\": season_period,\n",
    "                         \"corr_stl_resid\": stl_resid,\n",
    "                         \"corr_stl_trend_std\": stl_trend_std,\n",
    "                         \"corr_trend_coef\": coef_reg_trend,\n",
    "                         \"arima_model\": arima_model_name,\n",
    "                         \"arima_aic\": arima_aic,\n",
    "                         \"arima_arparams\": arima_arparams,\n",
    "                         \"arima_aroots\": arima_aroots,\n",
    "                         \"arima_maparams\": arima_maparams,\n",
    "                         \"arima_maroots\": arima_maroots,\n",
    "                         \"arima_params\": arima_params,\n",
    "                         \"arima_pvalues\": arima_pvalues,\n",
    "                         \"lstm_pred\": lstm_pred[0][0],\n",
    "                         \"arima_pred\": arima_pred[-1],\n",
    "                         \"hybrid_model_pred\": arima_pred[-1]+lstm_pred[0][0],\n",
    "                         \"ground_truth\": unseen_data_corr_df.iloc[0, -1],\n",
    "                         \"arima_err\": unseen_data_corr_df.iloc[0, -1] - arima_pred[-1],\n",
    "                         \"error\": (unseen_data_corr_df.iloc[0, -1] - (arima_pred[-1]+lstm_pred[0][0])),\n",
    "                         \"absolute_err\": math.copysign((unseen_data_corr_df.iloc[0, -1] - (arima_pred[-1]+lstm_pred[0][0])), 1),\n",
    "                         \"lstm_compensation_dir\": np.sign(unseen_data_corr_df.iloc[0, -1] - arima_pred[-1])*np.sign(lstm_pred[0][0])}\n",
    "\n",
    "        res_list.append(items_res_dic)\n",
    "        unseen_data_corr_df['items'] = f\"{items[0]} & {items[1]}\"\n",
    "        unseen_data_corr_df_concat = pd.concat([unseen_data_corr_df_concat, unseen_data_corr_df])\n",
    "        residual['items'] = f\"{items[0]} & {items[1]}\"\n",
    "        unseen_data_arima_resid_concat = pd.concat([unseen_data_arima_resid_concat, residual])\n",
    "\n",
    "if save_raw_corr_data:\n",
    "    unseen_data_corr_df_concat = unseen_data_corr_df_concat.set_index('items')\n",
    "    unseen_data_corr_df_concat.to_csv(f\"./results/{output_file_name}_raw_corr.csv\", index=True)\n",
    "\n",
    "if save_arima_resid_data:\n",
    "    unseen_data_arima_resid_concat = unseen_data_arima_resid_concat.set_index('items')\n",
    "    unseen_data_arima_resid_concat.to_csv(f\"./results/{output_file_name}_arima_resid.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810010d8-acca-45cb-b21b-2871ce2e5607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(res_list)\n",
    "res_df.to_csv(f\"./results/{output_file_name}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8daca-b55f-4a00-b162-4968bf696c50",
   "metadata": {},
   "source": [
    "# Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84895d7-ae0e-4cf9-9ecc-2adf76c38462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(f\"./results/{output_file_name}.csv\")\n",
    "display(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89180e3d-ee6c-4b6d-a05e-ea386f3f272f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def res_df_postprocess(target_df: pd.core.frame.DataFrame) -> None:\n",
    "    target_df['arima_pred_dir'] = np.sign(target_df['ground_truth'] * target_df['arima_pred'])\n",
    "    target_df['arima_err'] = target_df['ground_truth'] - target_df['arima_pred']\n",
    "    quantile_mask = np.logical_and(res_df['error'] < np.quantile(res_df['error'], 0.75), res_df['error'] > np.quantile(res_df['error'], 0.25)).tolist()\n",
    "    display(np.quantile(res_df['error'], 0.75), np.quantile(res_df['error'], 0.25))\n",
    "    target_df['high_pred_performance'] = quantile_mask\n",
    "    target_df['items[0]'] = target_df.apply(lambda row:row['items'].split(\" & \")[0], axis=1)\n",
    "    target_df['items[1]'] = target_df.apply(lambda row:row['items'].split(\" & \")[1], axis=1)\n",
    "\n",
    "\n",
    "res_df_postprocess(res_df)\n",
    "display(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7c64a-f3c7-4b5d-93bc-737c4ae4bf67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_exploration(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=7, ncols=2, sharex=False, sharey=False, dpi=100)\n",
    "    s0 = axes[0, 0]\n",
    "    s0.set_title(\"ABS_err violin\")\n",
    "    sns.violinplot(y=target_df[\"absolute_err\"], ax=s0)\n",
    "    s1 = axes[0, 1]\n",
    "    s1.set_title(\"Err violin\")\n",
    "    sns.violinplot(y=target_df[\"error\"], ax=s1)\n",
    "    s2 = axes[1, 0]\n",
    "    s2.set_title(\"ABS_err hist\")\n",
    "    target_df['absolute_err'].hist(bins=[b/10 for b in range(11)], ax=s2)\n",
    "    s3 = axes[1, 1]\n",
    "    s3.set_title(\"Err hist\")\n",
    "    target_df['error'].hist(bins=[b/10 for b in range(-10, 11)], ax=s3)\n",
    "    s4 = axes[2, 0]\n",
    "    s4.set_title(\"LSTM_compensation_dir count\")\n",
    "    sns.countplot(x=\"lstm_compensation_dir\", data=target_df, ax=s4)\n",
    "    s5 = axes[2, 1]\n",
    "    s5.set_title(\"LSTM_compensation_dir count groupby ARIMA_pred_dir\")\n",
    "    df_gb = target_df.groupby(['arima_pred_dir', 'lstm_compensation_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s5)\n",
    "    s6 = axes[3, 0]\n",
    "    s6.set_title(\"ARIMA_model prediction Err violin group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"arima_err\"], ax=s6)\n",
    "    s8 = axes[4, 0]\n",
    "    s8.set_title(\"ARIMA_model prediction magnitude group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"arima_pred\"], ax=s8)\n",
    "    s9 = axes[4, 1]\n",
    "    s9.set_title(\"LSTM compensation magnitude group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"lstm_pred\"], ax=s9)\n",
    "    s10 = axes[5, 0]\n",
    "    s10.set_title(\"Correlation magnitude in last period group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"ground_truth\"], ax=s10)\n",
    "    s11 = axes[5, 1]\n",
    "    s11.set_title(\"Hybrid Err violin group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"error\"], ax=s11)\n",
    "    s12 = axes[6,0]\n",
    "    s12.set_title(\"LSTM_compensation_dir pie with wrong ARIMA_pred_dir\")\n",
    "    df_gb.loc[df_gb.index==-1, :].squeeze().plot(kind=\"pie\", autopct='%1.1f%%', ax=s12)\n",
    "    s13 = axes[6,1]\n",
    "    s13.set_title(\"LSTM_compensation_dir pie with correct ARIMA_pred_dir\")\n",
    "    df_gb.loc[df_gb.index==1, :].squeeze().plot(kind=\"pie\", autopct='%1.1f%%', ax=s13)\n",
    "    \n",
    "    fig.suptitle(f\"{title}_basic_exploration\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./results/hybrid_prediction_analysis_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0699ee9-76fc-4804-8981-71997148f358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_exploration_pred_perform(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=6, ncols=2, sharex=False, sharey=False, dpi=100)\n",
    "    s1 = axes[0, 0]\n",
    "    s1.set_title(\"LSTM_compensation_dir count groupby prediction performance\")\n",
    "    df_gb = target_df.groupby(['high_pred_performance', 'lstm_compensation_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s1)\n",
    "    s2 = axes[0, 1]\n",
    "    s2.set_title(\"ARIMA_model prediction magnitude group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"arima_pred\"], ax=s2)\n",
    "    s3 = axes[1, 0]\n",
    "    s3.set_title(\"LSTM compensation magnitude group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"lstm_pred\"], ax=s3)\n",
    "    s4 = axes[1, 1]\n",
    "    s4.set_title(\"Correlation magnitude in last period group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"ground_truth\"], ax=s4)\n",
    "    s5 = axes[2, 0]\n",
    "    s5.set_title(\"Correlation series mean groupby prediction performance\")\n",
    "    sns.violinplot(x=target_df['high_pred_performance'], y=target_df[\"corr_ser_mean\"], ax=s5)\n",
    "    s6 = axes[2, 1]\n",
    "    s6.set_title(\"Correlation series std groupby prediction performance\")\n",
    "    sns.violinplot(x=target_df['high_pred_performance'], y=target_df[\"corr_ser_std\"], ax=s6)\n",
    "    s7 = axes[3, 0]\n",
    "    s7.set_title(\"Correlation series stl_period group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_season_period\"], ax=s7)\n",
    "    s8 = axes[3, 1]\n",
    "    s8.set_title(\"Correlation series stl_residual group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_resid\"], ax=s8)\n",
    "    s9 = axes[4, 0]\n",
    "    s9.set_title(\"Correlation series stl_trend_std group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_trend_std\"], ax=s9)\n",
    "    s10 = axes[4, 1]\n",
    "    s10.set_title(\"Correlation series stl_trend_coef group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_trend_coef\"], ax=s10)\n",
    "    s11 = axes[5, 0]\n",
    "    s11.set_title(\"ARIMA_pred_dir count groupby prediction performance\")\n",
    "    df_gb = target_df.groupby(['high_pred_performance', 'arima_pred_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s11)\n",
    "\n",
    "    fig.suptitle(F\"{title}_groupby prediction\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./results/hybrid_prediction_analysis_groupby_pred_perform_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751a5ad-6e92-40a3-915a-109ed10d8c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_stock_freq(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    stocks_show_freq = target_df.loc[target_df['high_pred_performance'] == True, ['items[0]','items[1]']].stack().value_counts().to_dict()\n",
    "    plt.figure(figsize=(80, 10), dpi=100)\n",
    "    plt.bar(range(len(stocks_show_freq)), list(stocks_show_freq.values()))\n",
    "    plt.xticks(range(len(stocks_show_freq)), list(stocks_show_freq.keys()), rotation=60)\n",
    "    plt.title(F\"{title}_items appearence frequence\")\n",
    "    plt.savefig(f\"./results/items_appearence_frequence_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c94193-1825-45f3-92fb-5d252cdae48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f\"mse :{(res_df['error']**2).mean()}\",\n",
    "        f\"std of square_err :{(res_df['error']**2).std()}\",\n",
    "        f\"rmse :{np.sqrt((res_df['error']**2).mean())}\",\n",
    "        f\"mae : {res_df['absolute_err'].mean()}\",\n",
    "        f\"std of abs_err: {res_df['absolute_err'].std()}\")\n",
    "\n",
    "display(f\"sklearn mse: {mean_squared_error(res_df['ground_truth'], res_df['hybrid_model_pred'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dda3ba-5e75-4610-8cf7-2de853713ce6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_exploration(res_df, fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c7e89-18db-498a-b3bd-2ad63a185935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_exploration_pred_perform(res_df, fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8151bfc-9a82-4854-a759-e422f3d579ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_stock_freq(res_df, fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0c2db-520c-4502-8e73-d2792c0898d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70447b37-8567-42b4-9bf0-12c9fbeb6578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
