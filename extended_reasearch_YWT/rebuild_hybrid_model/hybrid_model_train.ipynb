{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41660f84-97c9-4162-a708-f601b3b7a3a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.82 s (started: 2022-08-07 07:21:22 +00:00)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "import pathlib\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from pmdarima.arima import ARIMA, auto_arima\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from scipy import stats as st\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# from keras.models import Sequential, load_model\n",
    "# from keras.layers import Dense, LSTM, Activation\n",
    "# from keras import backend as K\n",
    "# from keras.utils.generic_utils import get_custom_objects\n",
    "# from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.data import Dataset\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a012b1-6376-486e-b5dd-93c76929fc3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dec99233-f96e-42c6-acc0-123e2ab6ceb6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 445 295\n",
      "time: 110 ms (started: 2022-08-07 07:21:25 +00:00)\n"
     ]
    }
   ],
   "source": [
    "stock_price_df = pd.read_csv(\"../../stock08_price.csv\")\n",
    "\n",
    "universe = list(stock_price_df.columns.values[1:])\n",
    "universe.remove(\"SP500\")\n",
    "# train data\n",
    "portfolio_train = ['CELG', 'PXD', 'WAT', 'LH', 'AMGN', 'AOS', 'EFX', 'CRM', 'NEM', 'JNPR', 'LB', 'CTAS', 'MAT', 'MDLZ', 'VLO', 'APH', 'ADM', 'MLM', 'BK', 'NOV', 'BDX', 'RRC', 'IVZ', 'ED', 'SBUX', 'GRMN', 'CI', 'ZION', 'COO', 'TIF', 'RHT', 'FDX', 'LLL', 'GLW', 'GPN', 'IPGP', 'GPC', 'HPQ', 'ADI', 'AMG', 'MTB', 'YUM', 'SYK', 'KMX', 'AME', 'AAP', 'DAL', 'A', 'MON', 'BRK', 'BMY', 'KMB', 'JPM', 'CCI', 'AET', 'DLTR', 'MGM', 'FL', 'HD', 'CLX', 'OKE', 'UPS', 'WMB', 'IFF', 'CMS', 'ARNC', 'VIAB', 'MMC', 'REG', 'ES', 'ITW', 'NDAQ', 'AIZ', 'VRTX', 'CTL', 'QCOM', 'MSI', 'NKTR', 'AMAT', 'BWA', 'ESRX', 'TXT', 'EXR', 'VNO', 'BBT', 'WDC', 'UAL', 'PVH', 'NOC', 'PCAR', 'NSC', 'UAA', 'FFIV', 'PHM', 'LUV', 'HUM', 'SPG', 'SJM', 'ABT', 'CMG', 'ALK', 'ULTA', 'TMK', 'TAP', 'SCG', 'CAT', 'TMO', 'AES', 'MRK', 'RMD', 'MKC', 'WU', 'ACN', 'HIG', 'TEL', 'DE', 'ATVI', 'O', 'UNM', 'VMC', 'ETFC', 'CMA', 'NRG', 'RHI', 'RE', 'FMC', 'MU', 'CB', 'LNT', 'GE', 'CBS', 'ALGN', 'SNA', 'LLY', 'LEN', 'MAA', 'OMC', 'F', 'APA', 'CDNS', 'SLG', 'HP', 'XLNX', 'SHW', 'AFL', 'STT', 'PAYX', 'AIG', 'FOX', 'MA']\n",
    "# all data\n",
    "portfolio_all = universe\n",
    "# all data - train data\n",
    "portfolio_other = [p for p in universe if p not in portfolio_train]\n",
    "print(len(portfolio_train), len(portfolio_all), len(portfolio_other))\n",
    "\n",
    "#paper_eva_info = {\"paper_eva_1\": {\"portfolio\": ['PRGO', 'MRO', 'ADP', 'HCP', 'FITB', 'PEG', 'SYMC', 'EOG', 'MDT', 'NI'], \"file_name\": \"paper_eva_1_res\"}, \n",
    "#                  \"paper_eva_2\": {\"portfolio\": ['STI', 'COP', 'MCD', 'AON', 'JBHT', 'DISH', 'GS', 'LRCX', 'CTXS', 'LEG'], \"file_name\": \"paper_eva_2_res\"},\n",
    "#                  \"paper_eva_3\": {\"portfolio\": ['TJX', 'EMN', 'JCI', 'C', 'BIIB', 'HOG', 'PX', 'PH', 'XEC', 'JEC'], \"file_name\": \"paper_eva_3_res\"},\n",
    "#                  \"paper_eva_4\": {\"portfolio\": ['ROP', 'AZO', 'URI', 'TROW', 'CMCSA', 'SLB', 'VZ', 'MAC', 'ADS', 'MCK'], \"file_name\": \"paper_eva_4_res\"},\n",
    "#                  \"paper_eva_5\": {\"portfolio\": ['RL', 'CVX', 'SRE', 'PFE', 'PCG', 'UTX', 'NTRS', 'INCY', 'COP', 'HRL'], \"file_name\": \"paper_eva_5_res\"},}\n",
    "#\n",
    "#paper_eva_implement = \"paper_eva_5\"\n",
    "#portfolio_implement = paper_eva_info[paper_eva_implement]['portfolio']\n",
    "#output_file_name = paper_eva_info[paper_eva_implement]['file_name']\n",
    "#fig_title = paper_eva_implement\n",
    "\n",
    "paper_eva_implement = \"150_train\"\n",
    "portfolio_implement = portfolio_train\n",
    "output_file_name = \"150_train_res\"\n",
    "fig_title = paper_eva_implement\n",
    "\n",
    "\n",
    "pd.to_datetime(stock_price_df['Date'], format='%Y-%m-%d')\n",
    "stock_price_df = stock_price_df.set_index(pd.DatetimeIndex(stock_price_df['Date']))\n",
    "# display(stock_price_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7765e10-2944-4621-b62f-6602a573387f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11175it [00:31, 351.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 34.8 s (started: 2022-08-07 07:26:03 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def gen_data_corr(portfolio: list, corr_ind: list) -> \"pd.DataFrame\":\n",
    "    tmp_corr = stock_price_df[portfolio[0]].rolling(window=100).corr(stock_price_df[portfolio[1]])\n",
    "    tmp_corr = tmp_corr.iloc[corr_ind].values\n",
    "    data_df = pd.DataFrame(tmp_corr.reshape(-1, 24))\n",
    "    ind = [f\"{portfolio[0]} & {portfolio[1]}_{i}\" for i in range(0, 100, 20)]\n",
    "    data_df.index = ind\n",
    "    return data_df\n",
    "\n",
    "\n",
    "def gen_train_data(portfolio: list, corr_ind: list, save_file: bool = False)-> \"four pd.DataFrame\":\n",
    "    train_df = pd.DataFrame()\n",
    "    dev_df = pd.DataFrame()\n",
    "    test1_df = pd.DataFrame()\n",
    "    test2_df = pd.DataFrame()\n",
    "\n",
    "    for pair in tqdm(combinations(portfolio, 2)):\n",
    "        data_df = gen_data_corr([pair[0], pair[1]], corr_ind=corr_ind)\n",
    "        data_split = {'train': [0, 21], 'dev': [1, 22], 'test1': [2, 23], 'test2': [3, 24]}\n",
    "        train_df = pd.concat([train_df, data_df.iloc[:, 0:21]])\n",
    "        dev_df = pd.concat([dev_df, data_df.iloc[:, 1:22]])\n",
    "        test1_df = pd.concat([test1_df, data_df.iloc[:, 2:23]])\n",
    "        test2_df = pd.concat([test2_df, data_df.iloc[:, 3:24]])\n",
    "\n",
    "    if save_file:\n",
    "        pathlib.Path('./dataset/before_arima/').mkdir(parents=True, exist_ok=True)\n",
    "        train_df.to_csv(\"./dataset/before_arima/train.csv\")\n",
    "        dev_df.to_csv(\"./dataset/before_arima/dev.csv\")\n",
    "        test1_df.to_csv(\"./dataset/before_arima/test1.csv\")\n",
    "        test2_df.to_csv(\"./dataset/before_arima/test2.csv\")\n",
    "\n",
    "    return train_df, dev_df, test1_df, test2_df \n",
    "\n",
    "\n",
    "corr_ind = list(range(99, 2400, 100)) + list(range(99+20, 2500, 100)) + list(range(99+40, 2500, 100)) + list(range(99+60, 2500, 100)) + list(range(99+80, 2500, 100))\n",
    "corr_datasets = gen_train_data(portfolio_implement, corr_ind, save_file = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7976949-33c7-45c0-b6e9-c8cace53672a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c7ee34a-a8f6-4609-9e69-cfa66bc19100",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 2.09 ms (started: 2022-08-07 07:26:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def arima_model(dataset: \"pd.DataFrame\", save_file_name: str = \"\") -> (\"pd.DataFrame\", \"pd.DataFrame\", \"pd.DataFrame\"):\n",
    "    model_110 = ARIMA(order=(1, 1, 0), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "    model_011 = ARIMA(order=(0, 1, 1), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "    model_111 = ARIMA(order=(1, 1, 1), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "    model_211 = ARIMA(order=(2, 1, 1), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "    model_210 = ARIMA(order=(2, 1, 0), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "    #model_330 = ARIMA(order=(3, 3, 0), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "\n",
    "    #model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210, \"model_330\": model_330}\n",
    "    model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "    tested_models = []\n",
    "    arima_model = None\n",
    "    find_arima_model = False\n",
    "    arima_model_name_list = []\n",
    "    arima_pred_list = []\n",
    "    residual = []\n",
    "    for s in dataset.iterrows():\n",
    "        while not find_arima_model:\n",
    "            try:\n",
    "                for model_key in model_dict:\n",
    "                    if model_key not in tested_models:\n",
    "                        test_model = model_dict[model_key].fit(s[1])\n",
    "                        if arima_model is None:\n",
    "                            arima_model = test_model\n",
    "                            arima_model_name = model_key\n",
    "                        elif arima_model.aic() <= test_model.aic():\n",
    "                            pass\n",
    "                        else:\n",
    "                            arima_model = test_model\n",
    "                            arima_model_name = model_key\n",
    "                    tested_models.append(model_key)\n",
    "\n",
    "            except Exception:\n",
    "                if len(model_dict)-1 != 0:\n",
    "                    del model_dict[model_key]\n",
    "                else:\n",
    "                    print(f\"fatal error, {s[1].name} doesn't have appropriate arima model\")\n",
    "                    break\n",
    "            else:\n",
    "                #model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210, \"model_330\": model_330}\n",
    "                model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "                tested_models.clear()\n",
    "                find_arima_model = True\n",
    "\n",
    "        arima_pred = list(arima_model.predict_in_sample())\n",
    "        arima_pred = [np.mean(arima_pred[1:])] + arima_pred[1:]\n",
    "        arima_pred = np.clip(np.array(arima_pred), -1, 1)\n",
    "\n",
    "        res = pd.Series(np.array(s[1]) - arima_pred)\n",
    "        arima_model_name_list.append(arima_model_name)\n",
    "        arima_pred_list.append(arima_pred)\n",
    "        residual.append(np.array(res))\n",
    "        find_arima_model = False\n",
    "    arima_model_name_df = pd.DataFrame(arima_model_name_list)\n",
    "    arima_pred_df = pd.DataFrame(arima_pred_list)\n",
    "    arima_resid_df = pd.DataFrame(residual)\n",
    "    arima_model_name_df.index = dataset.index\n",
    "    arima_pred_df.index = dataset.index\n",
    "    arima_resid_df.index = dataset.index\n",
    "\n",
    "    if save_file_name:\n",
    "        pathlib.Path('./dataset/after_arima').mkdir(parents=True, exist_ok=True)\n",
    "        arima_model_name_df.to_csv(f'./dataset/after_arima/arima_model_{save_file_name}.csv')\n",
    "        arima_pred_df.to_csv(f'./dataset/after_arima/arima_pred_{save_file_name}.csv')\n",
    "        arima_resid_df.to_csv(f'./dataset/after_arima/arima_resid_{save_file_name}.csv')\n",
    "\n",
    "    return arima_model_name_df, arima_pred_df, arima_resid_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "46a253c7-552c-4c6c-9072-8a6cc83297c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 10h 8min 3s (started: 2022-08-07 07:26:38 +00:00)\n"
     ]
    }
   ],
   "source": [
    "for (file_name, dataset) in tqdm(zip(['train', 'dev', 'test1', 'test2'], corr_datasets)):\n",
    "    arima_model(dataset, save_file_name=file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb455ad-4103-403d-a65f-ba8a13038295",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ed36f1c-62c7-4874-a3a4-1aaf35bcdc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.1 s (started: 2022-08-07 17:34:42 +00:00)\n"
     ]
    }
   ],
   "source": [
    "# Dataset.from_tensor_slices(dict(pd.read_csv(f'./dataset/after_arima/arima_resid_train.csv')))\n",
    "lstm_train_X = pd.read_csv(f'./dataset/after_arima/arima_resid_train.csv').set_index('Unnamed: 0').iloc[::, :-1]\n",
    "lstm_train_Y = pd.read_csv(f'./dataset/after_arima/arima_resid_train.csv').set_index('Unnamed: 0').iloc[::, -1]\n",
    "lstm_dev_X = pd.read_csv(f'./dataset/after_arima/arima_resid_dev.csv').set_index('Unnamed: 0').iloc[::, :-1]\n",
    "lstm_dev_Y = pd.read_csv(f'./dataset/after_arima/arima_resid_dev.csv').set_index('Unnamed: 0').iloc[::, -1]\n",
    "lstm_test1_X = pd.read_csv(f'./dataset/after_arima/arima_resid_test1.csv').set_index('Unnamed: 0').iloc[::, :-1]\n",
    "lstm_test1_Y = pd.read_csv(f'./dataset/after_arima/arima_resid_test1.csv').set_index('Unnamed: 0').iloc[::, -1]\n",
    "lstm_test2_X = pd.read_csv(f'./dataset/after_arima/arima_resid_test2.csv').set_index('Unnamed: 0').iloc[::, :-1]\n",
    "lstm_test2_Y = pd.read_csv(f'./dataset/after_arima/arima_resid_test2.csv').set_index('Unnamed: 0').iloc[::, -1]\n",
    "num_samples = len(lstm_train_X)\n",
    "\n",
    "lstm_train_X = lstm_train_X.values.reshape(num_samples, 20, 1)\n",
    "lstm_train_Y = lstm_train_Y.values.reshape(num_samples, 1)\n",
    "lstm_dev_X = lstm_dev_X.values.reshape(num_samples, 20, 1)\n",
    "lstm_dev_Y = lstm_dev_Y.values.reshape(num_samples, 1)\n",
    "lstm_test1_X = lstm_test1_X.values.reshape(num_samples, 20, 1)\n",
    "lstm_test1_Y = lstm_test1_Y.values.reshape(num_samples, 1)\n",
    "lstm_test2_X = lstm_test2_X.values.reshape(num_samples, 20, 1)\n",
    "lstm_test2_Y = lstm_test2_Y.values.reshape(num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8df49f64-a93a-4104-b07d-dfbd0a688004",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 17:34:43.300740: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-07 17:34:43.306122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-07 17:34:43.306404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-07 17:34:43.307254: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-08-07 17:34:43.308612: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-07 17:34:43.308899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-07 17:34:43.309145: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-07 17:34:43.623120: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-07 17:34:43.623288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-07 17:34:43.623413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-08-07 17:34:43.623511: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22308 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:0b:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"many_one_lstm\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20, 1)]           0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 25)                2700      \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 26        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,726\n",
      "Trainable params: 2,726\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "time: 653 ms (started: 2022-08-07 17:34:43 +00:00)\n"
     ]
    }
   ],
   "source": [
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) *2)\n",
    "\n",
    "\n",
    "def build_many_one_lstm():\n",
    "    inputs = Input(shape=(20, 1))\n",
    "    lstm_1 = LSTM(units=25, kernel_regularizer=l1_l2(0.0, 0.0), bias_regularizer=l1_l2(0.0, 0.0))(inputs)\n",
    "    outputs = Dense(units=1, activation=double_tanh)(lstm_1)\n",
    "    return keras.Model(inputs, outputs, name=\"many_one_lstm\")\n",
    "\n",
    "\n",
    "lstm_model = build_many_one_lstm()\n",
    "lstm_model.summary()\n",
    "lstm_model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa2b47f-ad31-431f-8ded-1e2acf98d7c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at models/epoch_1.h5",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Input \u001b[0;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch_num \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epoch_start\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m, max_epoch):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch_num \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 21\u001b[0m         lstm_model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpathlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepoch_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mepoch_num\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m.h5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdouble_tanh\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mdouble_tanh\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m     save_model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(pathlib\u001b[38;5;241m.\u001b[39mPath(model_dir)\u001b[38;5;241m/\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     24\u001b[0m                                                  monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m, save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     25\u001b[0m     lstm_model\u001b[38;5;241m.\u001b[39mfit(lstm_train_X, lstm_train_Y, epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, callbacks\u001b[38;5;241m=\u001b[39m[model_cbk, save_model])\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/saving/save.py:206\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    205\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[0;32m--> 206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    208\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(filepath_str, \u001b[38;5;28mcompile\u001b[39m, options)\n",
      "\u001b[0;31mOSError\u001b[0m: No file or directory found at models/epoch_1.h5"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 365 ms (started: 2022-08-07 17:34:43 +00:00)\n"
     ]
    }
   ],
   "source": [
    "model_dir = './models'\n",
    "log_dir = './models/lstm_train_logs'\n",
    "res_dir = './results'\n",
    "os.makedirs(res_dir, exist_ok=True)\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "res_csv_path = pathlib.Path(res_dir+'/LSTM_evaluation.csv')\n",
    "res_csv_path.touch(exist_ok=True)\n",
    "with open(res_csv_path, 'r+') as f:\n",
    "    if not f.read():\n",
    "        f.write(\"epoch,TRAIN_MSE,DEV_MSE,TEST1_MSE,TEST2_MSE,TRAIN_MAE,DEV_MAE,TEST1_MAE,TEST2_MAE\")\n",
    "\n",
    "res_df = pd.read_csv(res_csv_path)\n",
    "saved_model_list = [int(p.stem.split('_')[1]) for p in pathlib.Path(model_dir).glob('*.h5')]\n",
    "model_cbk = keras.callbacks.TensorBoard(log_dir=log_dir)\n",
    "epoch_start = max(saved_model_list) if saved_model_list else 1\n",
    "max_epoch = 3000\n",
    "\n",
    "for epoch_num in range(epoch_start, max_epoch):\n",
    "    if epoch_num > 1:\n",
    "        lstm_model = load_model(pathlib.Path(model_dir)/f\"epoch_{epoch_num - 1}.h5\", custom_objects={'double_tanh':double_tanh})\n",
    "\n",
    "    save_model = keras.callbacks.ModelCheckpoint(pathlib.Path(model_dir)/f\"epoch_{epoch_num}.h5\",\n",
    "                                                 monitor='loss', verbose=1, mode='min', save_best_only=False)\n",
    "    lstm_model.fit(lstm_train_X, lstm_train_Y, epochs=1, batch_size=500, shuffle=True, callbacks=[model_cbk, save_model])\n",
    "    \n",
    "    # test the model\n",
    "    score_train = lstm_model.evaluate(lstm_train_X, lstm_train_Y)\n",
    "    score_dev = lstm_model.evaluate(lstm_dev_X, lstm_dev_Y)\n",
    "    score_test1 = lstm_model.evaluate(lstm_test1_X, lstm_test1_Y)\n",
    "    score_test2 = lstm_model.evaluate(lstm_test2_X, lstm_test2_Y)\n",
    "    res_each_epoch_df = pd.DataFrame(np.array([epoch_num, score_train[0], score_dev[0], \n",
    "                                               score_test1[0], score_test2[0], \n",
    "                                               score_train[1], score_dev[1], \n",
    "                                               score_test1[1], score_test2[1]]).reshape(-1, 9),\n",
    "                                    columns=[\"epoch\", \"TRAIN_MSE\", \"DEV_MSE\", \"TEST1_MSE\", \n",
    "                                             \"TEST2_MSE\", \"TRAIN_MAE\", \"DEV_MAE\",\n",
    "                                             \"TEST1_MAE\",\"TEST2_MAE\"])\n",
    "    res_df = pd.concat([res_df, res_each_epoch_df])\n",
    "\n",
    "res_df.to_csv(res_csv_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2bb3d3-1516-4da3-ab8e-f27bd25b0945",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
