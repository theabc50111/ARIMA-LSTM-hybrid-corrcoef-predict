{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41660f84-97c9-4162-a708-f601b3b7a3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Logger concurrent.futures (INFO)>, <Logger concurrent (INFO)>, <Logger asyncio (INFO)>, <Logger tornado.access (INFO)>, <Logger tornado (INFO)>, <Logger tornado.application (INFO)>, <Logger tornado.general (INFO)>, <Logger stack_data.serializing (INFO)>, <Logger stack_data (INFO)>, <Logger prompt_toolkit.buffer (INFO)>, <Logger prompt_toolkit (INFO)>, <Logger parso.python.diff (INFO)>, <Logger parso.python (INFO)>, <Logger parso (INFO)>, <Logger parso.cache (INFO)>, <Logger pkg_resources.extern.packaging.tags (INFO)>, <Logger pkg_resources.extern.packaging (INFO)>, <Logger pkg_resources.extern (INFO)>, <Logger pkg_resources (INFO)>, <Logger IPKernelApp (DEBUG)>, <Logger tqdm.cli (INFO)>, <Logger tqdm (INFO)>, <Logger sklearn (INFO)>, <Logger setuptools.extern.packaging.tags (INFO)>, <Logger setuptools.extern.packaging (INFO)>, <Logger setuptools.extern (INFO)>, <Logger setuptools (INFO)>, <Logger setuptools.config._apply_pyprojecttoml (INFO)>, <Logger setuptools.config (INFO)>, <Logger setuptools.config.pyprojecttoml (INFO)>, <Logger PIL.Image (INFO)>, <Logger PIL (INFO)>, <Logger PIL.PngImagePlugin (INFO)>, <Logger matplotlib.ticker (ERROR)>, <Logger matplotlib (ERROR)>, <Logger matplotlib.artist (ERROR)>, <Logger matplotlib.lines (ERROR)>, <Logger matplotlib.dviread (ERROR)>, <Logger matplotlib.afm (ERROR)>, <Logger matplotlib.font_manager (ERROR)>, <Logger matplotlib.mathtext (ERROR)>, <Logger matplotlib.textpath (ERROR)>, <Logger matplotlib.backend_bases (ERROR)>, <Logger matplotlib.text (ERROR)>, <Logger matplotlib.colorbar (ERROR)>, <Logger matplotlib.image (ERROR)>, <Logger matplotlib.style.core (ERROR)>, <Logger matplotlib.style (ERROR)>, <Logger matplotlib.category (ERROR)>, <Logger matplotlib.dates (ERROR)>, <Logger matplotlib.axis (ERROR)>, <Logger matplotlib.axes._base (ERROR)>, <Logger matplotlib.axes (ERROR)>, <Logger matplotlib.axes._axes (ERROR)>, <Logger matplotlib.gridspec (ERROR)>, <Logger matplotlib.figure (ERROR)>, <Logger matplotlib.pyplot (ERROR)>, <Logger urllib3.util.retry (INFO)>, <Logger urllib3.util (INFO)>, <Logger urllib3 (INFO)>, <Logger urllib3.connection (INFO)>, <Logger urllib3.response (INFO)>, <Logger urllib3.connectionpool (INFO)>, <Logger urllib3.poolmanager (INFO)>, <ABSLLogger absl (INFO)>, <Logger tensorflow (INFO)>, <Logger charset_normalizer (INFO)>, <Logger requests (INFO)>, <Logger h5py._conv (INFO)>, <Logger h5py (INFO)>, <Logger arima_train_err (INFO)>]\n",
      "time: 70.3 ms (started: 2022-11-15 10:47:02 +00:00)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from itertools import combinations\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import warnings\n",
    "import logging\n",
    "from pprint import pformat\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pmdarima.arima import ARIMA, auto_arima\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "import dynamic_yaml\n",
    "import yaml\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "sys.path.append(\"/tf/correlation-coef-predict/ywt_library\")\n",
    "import data_generation\n",
    "from data_generation import data_gen_cfg\n",
    "from ywt_arima import arima_model, arima_err_logger_init\n",
    "from stl_decompn import stl_decompn\n",
    "from corr_property import calc_corr_ser_property\n",
    "\n",
    "with open('../config/data_config.yaml') as f:\n",
    "    data = dynamic_yaml.load(f)\n",
    "    data_cfg = yaml.full_load(dynamic_yaml.dump(data))\n",
    "\n",
    "warnings.simplefilter(\"ignore\")\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "matplotlib_logger = logging.getLogger(\"matplotlib\")\n",
    "matplotlib_logger.setLevel(logging.ERROR)\n",
    "mpl.rcParams[u'font.sans-serif'] = ['simhei']\n",
    "mpl.rcParams['axes.unicode_minus'] = False\n",
    "# logger_list = [logging.getLogger(name) for name in logging.root.manager.loggerDict]\n",
    "# print(logger_list)\n",
    "\n",
    "\n",
    "# %load_ext pycodestyle_magic\n",
    "# %pycodestyle_on --ignore E501\n",
    "logging.info(pformat(data_cfg, indent=1, width=100, compact=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a012b1-6376-486e-b5dd-93c76929fc3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ad3949-4662-4abc-8415-f86ed2ab03e1",
   "metadata": {},
   "source": [
    "## Data implement & output setting & testset setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4848bc85-9efc-4bc5-a63a-066bbc4d16ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_dir = Path('./results/')\n",
    "res_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# setting of output files\n",
    "save_corr_data = True\n",
    "save_arima_resid_data = True\n",
    "# data implement setting\n",
    "data_implement = \"SP500_20082017_FREQ_CLUSTER_LABEL_1\"  # watch options by operate: print(data_cfg[\"DATASETS\"].keys())\n",
    "# test set setting\n",
    "test_items_setting = \"-test_test\"  # -test_test|-test_all\n",
    "# data split period setting, only suit for only settings of Korean paper\n",
    "data_split_setting = \"-data_sp_test2\"\n",
    "# lstm weight setting\n",
    "lstm_weight_setting = \"SP500_20082017_KS_HYPER_LSTM\"  # watch options by operate: print(data_cfg[\"LSTM_WEIGHT\"].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b431c52f-e408-4b59-b512-ce32d8607d04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data loading & implement setting\n",
    "dataset_df = pd.read_csv(data_cfg[\"DATASETS\"][data_implement]['FILE_PATH'])\n",
    "dataset_df = dataset_df.set_index('Date')\n",
    "all_set = list(dataset_df.columns)  # all data\n",
    "train_set = data_cfg[\"DATASETS\"][data_implement]['TRAIN_SET']\n",
    "test_set = data_cfg['DATASETS'][data_implement]['TEST_SET'] if data_cfg['DATASETS'][data_implement].get('TEST_SET') else [p for p in all_set if p not in train_set]  # all data - train data\n",
    "logging.info(f\"===== len(train_set): {len(train_set)}, len(all_set): {len(all_set)}, len(test_set): {len(test_set)} =====\")\n",
    "\n",
    "# test items implement settings\n",
    "items_implement = test_set if test_items_setting == \"-test_test\" else all_set\n",
    "logging.info(f\"===== len(test set): {len(items_implement)} =====\")\n",
    "\n",
    "lstm_weight_filepath = data_cfg[\"LSTM_WEIGHT\"][lstm_weight_setting][\"FILE_PATH\"]\n",
    "lstm_weight_name = data_cfg[\"LSTM_WEIGHT\"][lstm_weight_setting][\"LSTM_WEIGHT_NAME\"]\n",
    "logging.info(f\"===== LSTM weight:{lstm_weight_name} =====\")\n",
    "\n",
    "# setting of name of output files and pictures title\n",
    "output_file_name = data_cfg[\"DATASETS\"][data_implement]['OUTPUT_FILE_NAME_BASIS'] + test_items_setting + lstm_weight_name\n",
    "fig_title = data_implement + test_items_setting + lstm_weight_name + data_split_setting\n",
    "logging.info(f\"===== file_name basis:{output_file_name}, fig_title basis:{fig_title} =====\")\n",
    "# display(dataset_df)\n",
    "# display(all_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ad0d4a-3be4-49f7-a49c-edda8854fa07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# corr_window = 100\n",
    "# corr_stride = 100\n",
    "# data_length = int(len(dataset_df)/corr_window)*corr_window\n",
    "# corr_ind = list(range(99, 2400, corr_stride))  + list(range(99+20, 2500, corr_stride)) + \\\n",
    "#            list(range(99+40, 2500, corr_stride)) + list(range(99+60, 2500, corr_stride)) + \\\n",
    "#            list(range(99+80, 2500, corr_stride))  # only suit for settings of paper\n",
    "\n",
    "# corr_series_length = int((data_length-corr_window)/corr_stride)\n",
    "# corr_series_length_paper = 21  # only suit for settings of paper\n",
    "# data_diverse_stride = 20  # only suit for settings of paper\n",
    "# data_diverse_start_dates = [99 + d for d in range(0,100,data_diverse_stride)]  # only suit for settings of paper\n",
    "# data_diverse_start_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5303e902-4c2a-43c2-86a7-cbc68d096625",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def gen_unseen_data_corr(item_pair: list, data_split_setting:str = \"_test2\") -> \"pd.DataFrame, pd.Series | pd.DataFrame\":\n",
    "#     tmp_corr = dataset_df[item_pair[0]].rolling(window=corr_window).corr(dataset_df[item_pair[1]])\n",
    "#     tmp_corr = tmp_corr.iloc[data_diverse_start_dates[0]::corr_stride]\n",
    "#     if data_split_setting == \"_test2\":\n",
    "#         corr_series = tmp_corr[3:24] # correspond to test2_dataset of original paper\n",
    "#     elif data_split_setting == \"_test1\" :\n",
    "#         corr_series = tmp_corr[2:23] # correspond to test1_dataset of original paper\n",
    "#     elif data_split_setting == \"_dev\":\n",
    "#         corr_series = tmp_corr[1:22] # correspond to dev_dataset of original paper\n",
    "#     elif data_split_setting == \"_train\":\n",
    "#         corr_series = tmp_corr[:21] # correspond to train_dataset of original papaer \n",
    "#     unseen_data_df = pd.DataFrame(corr_series).T\n",
    "#     unseen_data_df.index = [f\"{item_pair[0]} & {item_pair[1]}\"]\n",
    "#     return unseen_data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2a127e-cfe8-4035-b507-43406374dcf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display(gen_unseen_data_corr([items_implement[0], items_implement[1]], data_split_setting=\"_test2\"))\n",
    "# display(gen_unseen_data_corr([items_implement[0], items_implement[1]], data_split_setting=\"_test1\"))\n",
    "# display(gen_unseen_data_corr([items_implement[0], items_implement[1]], data_split_setting=\"_dev\"))\n",
    "# display(gen_unseen_data_corr([items_implement[0], items_implement[1]], data_split_setting=\"_train\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633e76df-1712-4ec8-a318-7b2e6bf556f6",
   "metadata": {},
   "source": [
    "## Load or Create Correlation Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dff757-38bd-4bc9-8926-6ea54ff2ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-corr_data\"\n",
    "corr_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "data_length = int(len(dataset_df)/data_gen_cfg[\"CORR_WINDOW\"])*data_gen_cfg[\"CORR_WINDOW\"]\n",
    "corr_ser_len_max = int((data_length-data_gen_cfg[\"CORR_WINDOW\"])/data_gen_cfg[\"CORR_STRIDE\"])\n",
    "max_data_div_start_add = 0  # In the Korea paper, each pair has 5 corr_series(due to diversifing train data).\n",
    "                            # BUT we only need to take one, so take 0 as arg.\n",
    "corr_ind = []\n",
    "\n",
    "# DEFAULT SETTING: data_gen_cfg[\"DATA_DIV_STRIDE\"] == 20, data_gen_cfg[\"CORR_WINDOW\"]==100, data_gen_cfg[\"CORR_STRIDE\"]==100\n",
    "data_end_init = corr_ser_len_max * data_gen_cfg[\"CORR_STRIDE\"]\n",
    "for i in range(0, max_data_div_start_add+1, data_gen_cfg[\"DATA_DIV_STRIDE\"]):\n",
    "    corr_ind.extend(list(range(data_gen_cfg[\"CORR_WINDOW\"]-1+i, data_end_init+bool(i)*data_gen_cfg[\"CORR_STRIDE\"], data_gen_cfg[\"CORR_STRIDE\"])))  # only suit for settings of paper\n",
    "\n",
    "train_df_path = corr_data_dir/f\"{output_file_name}-corr_train.csv\"\n",
    "dev_df_path = corr_data_dir/f\"{output_file_name}-corr_dev.csv\"\n",
    "test1_df_path = corr_data_dir/f\"{output_file_name}-corr_test1.csv\"\n",
    "test2_df_path = corr_data_dir/f\"{output_file_name}-corr_test2.csv\"\n",
    "all_corr_df_paths = dict(zip([\"train_df\", \"dev_df\", \"test1_df\", \"test2_df\"],\n",
    "                             [train_df_path, dev_df_path, test1_df_path, test2_df_path]))\n",
    "if all([df_path.exists() for df_path in all_corr_df_paths.values()]):\n",
    "    corr_datasets = [pd.read_csv(df_path).set_index(\"items\") for df_path in all_corr_df_paths.values()]\n",
    "else:\n",
    "    corr_datasets = data_generation.gen_train_data(items_implement, raw_data_df=dataset_df, corr_ser_len_max=corr_ser_len_max, corr_df_paths=all_corr_df_paths, corr_ind=corr_ind, max_data_div_start_add=max_data_div_start_add, save_file=save_corr_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd89d9f0-ec59-4066-bc6a-6d4ed2345869",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if data_split_setting == \"-data_sp_test2\":\n",
    "    corr_dataset = corr_datasets[3]\n",
    "print(corr_datasets[0].shape, corr_datasets[1].shape, corr_datasets[2].shape, corr_datasets[3].shape)\n",
    "display(corr_dataset.iloc[0,::])\n",
    "display(corr_dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7976949-33c7-45c0-b6e9-c8cace53672a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ARIMA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1bfef6-4dde-409f-aa4a-e04cdfad0079",
   "metadata": {},
   "outputs": [],
   "source": [
    "arima_result_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-arima_res\"\n",
    "arima_result_dir.mkdir(parents=True, exist_ok=True)\n",
    "arima_result_path_basis = arima_result_dir/f'{output_file_name}.csv'\n",
    "arima_result_types = [\"-arima_output\", \"-arima_resid\", \"-arima_model_info\"]\n",
    "arima_result_paths = []\n",
    "arima_err_logger_init(Path(os.path.abspath(''))/f\"results\")\n",
    "\n",
    "for arima_result_type in arima_result_types:\n",
    "    arima_result_paths.append(arima_result_dir/f'{output_file_name}{arima_result_type}{data_split_setting}.csv')\n",
    "\n",
    "if all([df_path.exists() for df_path in arima_result_paths]):\n",
    "    arima_output_df, arima_resid_df, arima_model_info_df = [pd.read_csv(arima_result_path, index_col=\"items\") for arima_result_path in arima_result_paths]\n",
    "else:\n",
    "    arima_output_df, arima_resid_df, arima_model_info_df = arima_model(corr_dataset, arima_result_path_basis=arima_result_path_basis, data_split_setting=data_split_setting, save_file=save_arima_resid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c7ee34a-a8f6-4609-9e69-cfa66bc19100",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def arima_model(dataset: \"pd.DataFrame\", portfolio: list, overview: bool = False) -> (\"np.array\", \"pd.DataFrame\", str):\n",
    "#     model_110 = ARIMA(order=(1, 1, 0), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "#     model_011 = ARIMA(order=(0, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "#     model_111 = ARIMA(order=(1, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "#     model_211 = ARIMA(order=(2, 1, 1), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "#     model_210 = ARIMA(order=(2, 1, 0), out_of_sample_size=10, mle_regression=True, suppress_warnings=True)\n",
    "#     # model_330 = ARIMA(order=(3, 3, 0), out_of_sample_size=0, mle_regression=True, suppress_warnings=True)\n",
    "\n",
    "#     # model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111}\n",
    "#     model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "#     tested_models = []\n",
    "#     arima_model = None\n",
    "#     find_arima_model = False\n",
    "#     arima_attr_list = [\"aic\", \"arparams\", \"aroots\", \"maparams\", \"maroots\", \"params\", \"pvalues\"]\n",
    "#     arima_infos = dict(zip(arima_attr_list, [None]*len(arima_attr_list)))\n",
    "#     for _, corr_series in dataset.iterrows():\n",
    "#         while not find_arima_model:\n",
    "#             try:\n",
    "#                 for model_key in model_dict:\n",
    "#                     if model_key not in tested_models:\n",
    "#                         test_model = model_dict[model_key].fit(corr_series[:-1])  # only use first 20 corrletaion coefficient to fit ARIMA model\n",
    "#                         if arima_model is None:\n",
    "#                             arima_model = test_model\n",
    "#                             arima_model_name = model_key\n",
    "#                         elif arima_model.aic() <= test_model.aic():\n",
    "#                             pass\n",
    "#                         else:\n",
    "#                             arima_model = test_model\n",
    "#                             arima_model_name = model_key\n",
    "#                     tested_models.append(model_key)\n",
    "#             except Exception:\n",
    "#                 if len(model_dict)-1 != 0:\n",
    "#                     del model_dict[model_key]\n",
    "#                 else:\n",
    "#                     err_logger.error(f\"fatal error, {portfolio} doesn't have appropriate arima model\\n\", exc_info=True)\n",
    "#                     raise NotImplementedError(f\"fatal error, {portfolio} doesn't have appropriate arima model\\n\")\n",
    "#             else:\n",
    "#                 # model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210, \"model_330\": model_330}\n",
    "#                 model_dict = {\"model_110\": model_110, \"model_011\": model_011, \"model_111\": model_111, \"model_211\": model_211, \"model_210\": model_210}\n",
    "#                 tested_models.clear()\n",
    "#                 find_arima_model = True\n",
    "#         try:\n",
    "#             arima_pred = list(arima_model.predict(n_periods=1))\n",
    "#         except Exception:\n",
    "#             err_logger.error(f\"{portfolio} in {data_split_setting} be predicted by {arima_model_name}(its aic:{arima_model.aic()}) getting error:\\n\", exc_info=True)\n",
    "#             raise NotImplementedError(f\"{portfolio} in {data_split_setting} be predicted by {arima_model_name}(its aic:{arima_model.aic()}) getting error\\n\")\n",
    "#         else:\n",
    "#             arima_pred_in_sample = list(arima_model.predict_in_sample())\n",
    "#             arima_pred_in_sample = [np.mean(arima_pred_in_sample[1:])] + arima_pred_in_sample[1:]\n",
    "#             arima_output = arima_pred_in_sample + arima_pred\n",
    "#             arima_output = np.clip(np.array(arima_output), -1, 1)\n",
    "\n",
    "#             arima_resid = pd.Series(np.array(corr_series) - arima_output).iloc[:-1]\n",
    "\n",
    "#             for attr in arima_infos.keys():\n",
    "#                 try:\n",
    "#                     arima_infos[attr] = getattr(arima_model, attr)()\n",
    "#                 except AttributeError:\n",
    "#                     pass\n",
    "#         finally:\n",
    "#             find_arima_model = False\n",
    "#     if overview:\n",
    "#         plt.plot(arima_output, label=\"arima_pred\")\n",
    "#         plt.plot(dataset.T, label=\"data\")\n",
    "#         plt.plot(arima_resid, label=\"res\")\n",
    "#         plt.legend()\n",
    "#         plt.show()\n",
    "#         plt.close()\n",
    "\n",
    "#     return arima_output, arima_resid, arima_model_name, *[v for k, v in sorted(arima_infos.items(), key=lambda x:x[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50ab82-3053-41e2-b20c-d2a4bcb0fe85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unseen_data_corr_df = gen_unseen_data_corr([items_implement[0], items_implement[1]], data_split_setting=\"_test2\")\n",
    "# arima_pred, residual, arima_model_name, arima_aic, arima_arparams, arima_aroots, arima_maparams, arima_maroots, arima_params, arima_pvalues = arima_model(unseen_data_corr_df, [items_implement[0], items_implement[1]], overview=True)\n",
    "# display(len(arima_pred), len(residual))\n",
    "# display(arima_pred, residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfb455ad-4103-403d-a65f-ba8a13038295",
   "metadata": {
    "tags": []
   },
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35591071-44de-4a02-8dfa-fb85bb7ae29d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def double_tanh(x):\n",
    "    return (tf.math.tanh(x) * 2)\n",
    "\n",
    "\n",
    "lstm_model = load_model(lstm_weight_filepath, custom_objects={'double_tanh':double_tanh})\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65707dce-375f-4d15-84fd-e4edf9133cac",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32998ac-361f-427f-aaf4-4667f8a6acb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_input = arima_resid_df.iloc[::,:-1].values.reshape(-1, 20, 1)\n",
    "lstm_pred = lstm_model.predict(lstm_input)\n",
    "lstm_pred = pd.DataFrame(lstm_pred, index=arima_resid_df.index, columns=[\"lstm_pred\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f113b850-5406-481e-bcbd-a6831edb95c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# res_list = []\n",
    "# unseen_data_corr_df_concat = pd.DataFrame()\n",
    "# unseen_data_arima_resid_concat = pd.DataFrame()\n",
    "# count = 0\n",
    "# for items in tqdm(combinations(items_implement, 2)):\n",
    "#     unseen_data_corr_df = gen_unseen_data_corr(items, data_split_setting=data_split_setting)\n",
    "#     try:\n",
    "#         # arima_pred, residual, arima_model_name, arima_aic, arima_arparams, arima_aroots, arima_maparams, arima_maroots, arima_params, arima_pvalues = arima_model(unseen_data_corr_df, items)\n",
    "#         pass\n",
    "#     except NotImplementedError:\n",
    "#         continue\n",
    "#     else:\n",
    "#         unseen_res = residual.values.reshape((-1, 20, 1))\n",
    "#         lstm_pred = lstm_model.predict(unseen_res)\n",
    "#         # season_period, stl_resid, stl_trend_std, coef_reg_trend = stl_decompn(unseen_data_corr_df.iloc[0, :])\n",
    "#         items_res_dic = {\"items\": f\"{items[0]} & {items[1]}\",\n",
    "#                          \"corr_ser_mean\": unseen_data_corr_df.mean(axis=1)[0],\n",
    "#                          \"corr_ser_std\": unseen_data_corr_df.std(axis=1)[0],\n",
    "#                          \"corr_season_period\": season_period,\n",
    "#                          \"corr_stl_resid\": stl_resid,\n",
    "#                          \"corr_stl_trend_std\": stl_trend_std,\n",
    "#                          \"corr_trend_coef\": coef_reg_trend,\n",
    "#                          \"arima_model\": arima_model_name,\n",
    "#                          \"arima_aic\": arima_aic,\n",
    "#                          \"arima_arparams\": arima_arparams,\n",
    "#                          \"arima_aroots\": arima_aroots,\n",
    "#                          \"arima_maparams\": arima_maparams,\n",
    "#                          \"arima_maroots\": arima_maroots,\n",
    "#                          \"arima_params\": arima_params,\n",
    "#                          \"arima_pvalues\": arima_pvalues,\n",
    "#                          \"lstm_pred\": lstm_pred[0][0],\n",
    "#                          \"arima_pred\": arima_pred[-1],\n",
    "#                          \"ground_truth\": unseen_data_corr_df.iloc[0, -1]}\n",
    "\n",
    "#         res_list.append(items_res_dic)\n",
    "#         unseen_data_corr_df['items'] = f\"{items[0]} & {items[1]}\"\n",
    "#         unseen_data_corr_df_concat = pd.concat([unseen_data_corr_df_concat, unseen_data_corr_df])\n",
    "#         residual['items'] = f\"{items[0]} & {items[1]}\"\n",
    "#         unseen_data_arima_resid_concat = pd.concat([unseen_data_arima_resid_concat, residual], axis=1)\n",
    "\n",
    "# if save_raw_corr_data:\n",
    "#     unseen_data_corr_df_concat = unseen_data_corr_df_concat.set_index('items')\n",
    "#     unseen_data_corr_df_concat.to_csv(f\"./results/{output_file_name}-raw_corr.csv\", index=True)\n",
    "\n",
    "# if save_arima_resid_data:\n",
    "#     unseen_data_arima_resid_concat = unseen_data_arima_resid_concat.T.set_index('items')\n",
    "#     unseen_data_arima_resid_concat.to_csv(f\"./results/{output_file_name}-arima_resid.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc683e2-8879-4fb2-ba95-50403889c18d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Results post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1924c24c-df58-4521-9f33-5f8a0fe8124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def res_df_postprocess(merge_dfs: list) -> \"pd.DataFrame\":\n",
    "\n",
    "    tmp_df = pd.DataFrame(columns=[\"items\"])\n",
    "    for merge_df in merge_dfs:\n",
    "        tmp_df = pd.merge(tmp_df, merge_df, on=\"items\", how='outer')\n",
    "    else:\n",
    "        results_df = tmp_df.reset_index(drop=True)\n",
    "\n",
    "    results_df[\"hybrid_model_pred\"] =  results_df[\"arima_pred\"] + results_df[\"lstm_pred\"]\n",
    "    results_df[\"error\"] = results_df[\"ground_truth\"] - results_df[\"hybrid_model_pred\"]\n",
    "    results_df[\"absolute_err\"] = results_df[\"error\"].abs()\n",
    "    results_df['arima_pred_dir'] = np.sign(results_df['ground_truth'] * results_df['arima_pred'])\n",
    "    results_df['arima_err'] = results_df['ground_truth'] - results_df['arima_pred']\n",
    "    results_df[\"lstm_compensation_dir\"] = np.sign(results_df['arima_err']) * np.sign(results_df['lstm_pred'])\n",
    "    quantile_mask = np.logical_and(results_df['error'] < np.quantile(results_df['error'], 0.75), results_df['error'] > np.quantile(results_df['error'], 0.25)).tolist()\n",
    "    # display(np.quantile(res_df['error'], 0.75), np.quantile(res_df['error'], 0.25))\n",
    "    results_df['high_pred_performance'] = quantile_mask\n",
    "    results_df['items[0]'] = results_df.apply(lambda row:row['items'].split(\" & \")[0], axis=1)\n",
    "    results_df['items[1]'] = results_df.apply(lambda row:row['items'].split(\" & \")[1][:-2], axis=1)\n",
    "    results_df = results_df.set_index(\"items\")\n",
    "\n",
    "    return results_df\n",
    "\n",
    "\n",
    "# stl_decompn(corr_datasets[0].iloc[0,::], overview=True)\n",
    "corr_property_dir = Path(data_cfg[\"DIRS\"][\"PIPELINE_DATA_DIR\"])/f\"{output_file_name}-corr_property\"\n",
    "corr_property_dir.mkdir(parents=True, exist_ok=True)\n",
    "corr_property_df_path = corr_property_dir/f\"{output_file_name}{data_split_setting}-corr_series_property.csv\"\n",
    "\n",
    "if corr_property_df_path.exists():\n",
    "    corr_property_df = pd.read_csv(corr_property_df_path)\n",
    "else:\n",
    "    corr_property_df = calc_corr_ser_property(corr_dataset=corr_dataset, corr_property_df_path=corr_property_df_path)\n",
    "\n",
    "ground_truth = corr_dataset.iloc[::, -1]\n",
    "ground_truth.name = \"ground_truth\"\n",
    "arima_pred = arima_output_df.iloc[::, -1]\n",
    "arima_pred.name = \"arima_pred\"\n",
    "merge_dfs = [arima_model_info_df, corr_property_df, arima_pred, lstm_pred, ground_truth]\n",
    "\n",
    "res_df = res_df_postprocess(merge_dfs)\n",
    "res_df.to_csv(res_dir/f\"{output_file_name}{data_split_setting}-res.csv\")\n",
    "\n",
    "# def res_df_postprocess(target_df: pd.core.frame.DataFrame) -> None:\n",
    "#     target_df[\"hybrid_model_pred\"] =  target_df[\"arima_pred\"] + target_df[\"lstm_pred\"]\n",
    "#     target_df[\"error\"] = target_df[\"ground_truth\"] - target_df[\"hybrid_model_pred\"]\n",
    "#     target_df[\"absolute_err\"] = target_df[\"error\"].abs()\n",
    "#     target_df['arima_pred_dir'] = np.sign(target_df['ground_truth'] * target_df['arima_pred'])\n",
    "#     target_df['arima_err'] = target_df['ground_truth'] - target_df['arima_pred']\n",
    "#     target_df[\"lstm_compensation_dir\"] = np.sign(target_df['arima_err']) * np.sign(target_df['lstm_pred'])\n",
    "#     quantile_mask = np.logical_and(res_df['error'] < np.quantile(res_df['error'], 0.75), res_df['error'] > np.quantile(res_df['error'], 0.25)).tolist()\n",
    "#     display(np.quantile(res_df['error'], 0.75), np.quantile(res_df['error'], 0.25))\n",
    "#     target_df['high_pred_performance'] = quantile_mask\n",
    "#     target_df['items[0]'] = target_df.apply(lambda row:row['items'].split(\" & \")[0], axis=1)\n",
    "#     target_df['items[1]'] = target_df.apply(lambda row:row['items'].split(\" & \")[1], axis=1)\n",
    "\n",
    "\n",
    "# res_df_postprocess(res_df)\n",
    "# display(res_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d8daca-b55f-4a00-b162-4968bf696c50",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Display results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84895d7-ae0e-4cf9-9ecc-2adf76c38462",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_df = pd.read_csv(res_dir/f\"{output_file_name}{data_split_setting}-res.csv\", index_col=[\"items\"])\n",
    "display(res_df.columns)\n",
    "display(res_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b7c64a-f3c7-4b5d-93bc-737c4ae4bf67",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_exploration(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=7, ncols=2, sharex=False, sharey=False, dpi=100)\n",
    "    s0 = axes[0, 0]\n",
    "    s0.set_title(\"ABS_err violin\")\n",
    "    sns.violinplot(y=target_df[\"absolute_err\"], ax=s0)\n",
    "    s1 = axes[0, 1]\n",
    "    s1.set_title(\"Err violin\")\n",
    "    sns.violinplot(y=target_df[\"error\"], ax=s1)\n",
    "    s2 = axes[1, 0]\n",
    "    s2.set_title(\"ABS_err hist\")\n",
    "    target_df['absolute_err'].hist(bins=[b/10 for b in range(11)], ax=s2)\n",
    "    s3 = axes[1, 1]\n",
    "    s3.set_title(\"Err hist\")\n",
    "    target_df['error'].hist(bins=[b/10 for b in range(-10, 11)], ax=s3)\n",
    "    s4 = axes[2, 0]\n",
    "    s4.set_title(\"LSTM_compensation_dir count\")\n",
    "    sns.countplot(x=\"lstm_compensation_dir\", data=target_df, ax=s4)\n",
    "    s5 = axes[2, 1]\n",
    "    s5.set_title(\"LSTM_compensation_dir count groupby ARIMA_pred_dir\")\n",
    "    df_gb = target_df.groupby(['arima_pred_dir', 'lstm_compensation_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s5)\n",
    "    s6 = axes[3, 0]\n",
    "    s6.set_title(\"ARIMA_model prediction Err violin group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"arima_err\"], ax=s6)\n",
    "    s8 = axes[4, 0]\n",
    "    s8.set_title(\"ARIMA_model prediction magnitude group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"arima_pred\"], ax=s8)\n",
    "    s9 = axes[4, 1]\n",
    "    s9.set_title(\"LSTM compensation magnitude group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"lstm_pred\"], ax=s9)\n",
    "    s10 = axes[5, 0]\n",
    "    s10.set_title(\"Correlation magnitude in last period group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"ground_truth\"], ax=s10)\n",
    "    s11 = axes[5, 1]\n",
    "    s11.set_title(\"Hybrid Err violin group by LSTM_compensation_dir\")\n",
    "    sns.violinplot(x=target_df[\"lstm_compensation_dir\"], y=target_df[\"error\"], ax=s11)\n",
    "    s12 = axes[6,0]\n",
    "    s12.set_title(\"LSTM_compensation_dir pie with wrong ARIMA_pred_dir\")\n",
    "    df_gb.loc[df_gb.index==-1, :].squeeze().plot(kind=\"pie\", autopct='%1.1f%%', ax=s12)\n",
    "    s13 = axes[6,1]\n",
    "    s13.set_title(\"LSTM_compensation_dir pie with correct ARIMA_pred_dir\")\n",
    "    df_gb.loc[df_gb.index==1, :].squeeze().plot(kind=\"pie\", autopct='%1.1f%%', ax=s13)\n",
    "\n",
    "    fig.suptitle(f\"{title}_basic_exploration\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./results/hybrid_prediction_analysis_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0699ee9-76fc-4804-8981-71997148f358",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_exploration_pred_perform(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    fig, axes = plt.subplots(figsize=(20, 20), nrows=6, ncols=2, sharex=False, sharey=False, dpi=100)\n",
    "    s1 = axes[0, 0]\n",
    "    s1.set_title(\"LSTM_compensation_dir count groupby prediction performance\")\n",
    "    df_gb = target_df.groupby(['high_pred_performance', 'lstm_compensation_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s1)\n",
    "    s2 = axes[0, 1]\n",
    "    s2.set_title(\"ARIMA_model prediction magnitude group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"arima_pred\"], ax=s2)\n",
    "    s3 = axes[1, 0]\n",
    "    s3.set_title(\"LSTM compensation magnitude group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"lstm_pred\"], ax=s3)\n",
    "    s4 = axes[1, 1]\n",
    "    s4.set_title(\"Correlation magnitude in last period group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"ground_truth\"], ax=s4)\n",
    "    s5 = axes[2, 0]\n",
    "    s5.set_title(\"Correlation series mean groupby prediction performance\")\n",
    "    sns.violinplot(x=target_df['high_pred_performance'], y=target_df[\"corr_ser_mean\"], ax=s5)\n",
    "    s6 = axes[2, 1]\n",
    "    s6.set_title(\"Correlation series std groupby prediction performance\")\n",
    "    sns.violinplot(x=target_df['high_pred_performance'], y=target_df[\"corr_ser_std\"], ax=s6)\n",
    "    s7 = axes[3, 0]\n",
    "    s7.set_title(\"Correlation series stl_period group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_period\"], ax=s7)\n",
    "    s8 = axes[3, 1]\n",
    "    s8.set_title(\"Correlation series stl_residual group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_resid\"], ax=s8)\n",
    "    s9 = axes[4, 0]\n",
    "    s9.set_title(\"Correlation series stl_trend_std group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_trend_std\"], ax=s9)\n",
    "    s10 = axes[4, 1]\n",
    "    s10.set_title(\"Correlation series stl_trend_coef group by prediction performance\")\n",
    "    sns.violinplot(x=target_df[\"high_pred_performance\"], y=target_df[\"corr_stl_trend_coef\"], ax=s10)\n",
    "    s11 = axes[5, 0]\n",
    "    s11.set_title(\"ARIMA_pred_dir count groupby prediction performance\")\n",
    "    df_gb = target_df.groupby(['high_pred_performance', 'arima_pred_dir']).size().unstack(level=1)\n",
    "    df_gb.plot(kind='bar', ax=s11)\n",
    "\n",
    "    fig.suptitle(F\"{title}_groupby prediction\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"./results/hybrid_prediction_analysis_groupby_pred_perform_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9751a5ad-6e92-40a3-915a-109ed10d8c05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_stock_freq(target_df: pd.core.frame.DataFrame, title: str) -> None:\n",
    "    stocks_show_freq = target_df.loc[target_df['high_pred_performance'] == True, ['items[0]','items[1]']].stack().value_counts().to_dict()\n",
    "    plt.figure(figsize=(80, 10), dpi=100)\n",
    "    plt.bar(range(len(stocks_show_freq)), list(stocks_show_freq.values()))\n",
    "    plt.xticks(range(len(stocks_show_freq)), list(stocks_show_freq.keys()), rotation=60)\n",
    "    plt.title(F\"{title}_items appearence frequence\")\n",
    "    plt.savefig(f\"./results/items_appearence_frequence_{title}.png\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c94193-1825-45f3-92fb-5d252cdae48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(f\"mse :{(res_df['error']**2).mean()}\",\n",
    "        f\"std of square_err :{(res_df['error']**2).std()}\",\n",
    "        f\"rmse :{np.sqrt((res_df['error']**2).mean())}\",\n",
    "        f\"mae : {res_df['absolute_err'].mean()}\",\n",
    "        f\"std of abs_err: {res_df['absolute_err'].std()}\",\n",
    "        f\"sklearn mse: {mean_squared_error(res_df['ground_truth'], res_df['hybrid_model_pred'])}\")\n",
    "display(\"-\"*50)\n",
    "display(f\"mse of ARIMA :{(res_df['arima_err']**2).mean()}\",\n",
    "        f\"std of square_err ARIMA :{(res_df['arima_err']**2).std()}\",\n",
    "        f\"rmse of ARIMA :{np.sqrt((res_df['arima_err']**2).mean())}\",\n",
    "        f\"sklearn mse of ARIMA: {mean_squared_error(res_df['ground_truth'], res_df['arima_pred'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d1c7e89-18db-498a-b3bd-2ad63a185935",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_exploration(res_df, fig_title)\n",
    "plot_exploration_pred_perform(res_df, fig_title)\n",
    "plot_stock_freq(res_df, fig_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8151bfc-9a82-4854-a759-e422f3d579ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f0c2db-520c-4502-8e73-d2792c0898d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70447b37-8567-42b4-9bf0-12c9fbeb6578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
